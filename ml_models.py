"""
ü§ñ MUZAQUEST ANALYTICS - –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞
"""

import pandas as pd
import numpy as np
import sqlite3
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from sklearn.ensemble import RandomForestRegressor, IsolationForest
    from sklearn.linear_model import LinearRegression
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_absolute_error, r2_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ scikit-learn: pip install scikit-learn")

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Prophet: pip install prophet")

class RestaurantMLAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, db_path='database.sqlite'):
        self.db_path = db_path
        self.scaler = StandardScaler()
        self.models = {}
        
    def load_restaurant_data(self, restaurant_id, start_date=None, end_date=None):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞"""
        
        conn = sqlite3.connect(self.db_path)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ Grab –∏ Gojek
        query = """
        WITH combined_data AS (
            SELECT 
                g.stat_date,
                g.rating as grab_rating,
                g.sales as grab_sales,
                g.orders as grab_orders,
                g.ads_spend,
                g.ads_sales,
                g.new_customers,
                g.repeated_customers,
                g.store_is_closed,
                g.out_of_stock,
                g.ads_ctr,
                g.impressions,
                gj.rating as gojek_rating,
                gj.sales as gojek_sales,
                gj.orders as gojek_orders,
                gj.realized_orders_percentage,
                gj.one_star_ratings,
                gj.five_star_ratings
            FROM grab_stats g
            LEFT JOIN gojek_stats gj ON g.restaurant_id = gj.restaurant_id 
                                     AND g.stat_date = gj.stat_date
            WHERE g.restaurant_id = ?
        )
        SELECT 
            stat_date,
            COALESCE(grab_rating, gojek_rating, 4.5) as rating,
            (COALESCE(grab_sales, 0) + COALESCE(gojek_sales, 0)) as total_sales,
            (COALESCE(grab_orders, 0) + COALESCE(gojek_orders, 0)) as total_orders,
            COALESCE(ads_spend, 0) as marketing_spend,
            COALESCE(ads_sales, 0) as marketing_sales,
            COALESCE(new_customers, 0) as new_customers,
            COALESCE(repeated_customers, 0) as repeat_customers,
            COALESCE(store_is_closed, 0) as is_closed,
            COALESCE(out_of_stock, 0) as out_of_stock,
            COALESCE(ads_ctr, 0) as ctr,
            COALESCE(impressions, 0) as impressions,
            COALESCE(realized_orders_percentage, 90) as order_completion,
            COALESCE(one_star_ratings, 0) as negative_reviews,
            COALESCE(five_star_ratings, 0) as positive_reviews
        FROM combined_data
        ORDER BY stat_date
        """
        
        df = pd.read_sql_query(query, conn, params=[restaurant_id])
        conn.close()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df['stat_date'] = pd.to_datetime(df['stat_date'])
        df = df.sort_values('stat_date')
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–∞–º
        if start_date:
            df = df[df['stat_date'] >= start_date]
        if end_date:
            df = df[df['stat_date'] <= end_date]
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self._create_features(df)
        
        return df
    
    def _create_features(self, df):
        """–°–æ–∑–¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML"""
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['day_of_week'] = df['stat_date'].dt.dayofweek
        df['month'] = df['stat_date'].dt.month
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        df['sales_lag_1'] = df['total_sales'].shift(1)
        df['sales_lag_7'] = df['total_sales'].shift(7)
        df['rating_lag_1'] = df['rating'].shift(1)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        df['sales_ma_7'] = df['total_sales'].rolling(window=7, min_periods=1).mean()
        df['sales_ma_30'] = df['total_sales'].rolling(window=30, min_periods=1).mean()
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        df['avg_order_value'] = df['total_sales'] / (df['total_orders'] + 1)
        df['marketing_efficiency'] = df['marketing_sales'] / (df['marketing_spend'] + 1)
        df['customer_retention'] = df['repeat_customers'] / (df['new_customers'] + df['repeat_customers'] + 1)
        
        # –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['operational_issues'] = df['is_closed'] + df['out_of_stock']
        
        return df
    
    def train_sales_prediction_model(self, df, model_type='random_forest'):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂"""
        
        if not SKLEARN_AVAILABLE:
            return None, "Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
            
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_columns = [
            'rating', 'total_orders', 'marketing_spend', 'new_customers', 
            'repeat_customers', 'day_of_week', 'month', 'is_weekend',
            'sales_lag_1', 'sales_lag_7', 'avg_order_value', 
            'marketing_efficiency', 'customer_retention', 'operational_issues',
            'order_completion', 'negative_reviews', 'positive_reviews'
        ]
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df_clean = df.dropna(subset=feature_columns + ['total_sales'])
        
        if len(df_clean) < 10:
            return None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            
        X = df_clean[feature_columns]
        y = df_clean['total_sales']
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        if model_type == 'linear':
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
        else:  # random_forest
            model = RandomForestRegressor(
                n_estimators=100, 
                max_depth=10, 
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_train, y_train)  # RF –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        if model_type == 'linear':
            y_pred = model.predict(X_test_scaled)
        else:
            y_pred = model.predict(X_test)
            
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è Random Forest)
        feature_importance = None
        if model_type == 'random_forest':
            feature_importance = dict(zip(feature_columns, model.feature_importances_))
            feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        self.models['sales_prediction'] = {
            'model': model,
            'type': model_type,
            'features': feature_columns,
            'mae': mae,
            'r2': r2,
            'feature_importance': feature_importance
        }
        
        return model, {
            'mae': mae,
            'r2': r2,
            'feature_importance': feature_importance
        }
    
    def detect_anomalies(self, df, contamination=0.1):
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π —Å –ø–æ–º–æ—â—å—é Isolation Forest"""
        
        if not SKLEARN_AVAILABLE:
            return None, "Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
            
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        anomaly_features = [
            'total_sales', 'total_orders', 'rating', 'marketing_spend',
            'avg_order_value', 'customer_retention', 'operational_issues'
        ]
        
        df_clean = df.dropna(subset=anomaly_features)
        
        if len(df_clean) < 10:
            return None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"
            
        X = df_clean[anomaly_features]
        X_scaled = self.scaler.fit_transform(X)
        
        # Isolation Forest
        iso_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_jobs=-1
        )
        
        anomaly_labels = iso_forest.fit_predict(X_scaled)
        anomaly_scores = iso_forest.score_samples(X_scaled)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
        df_clean = df_clean.copy()
        df_clean['is_anomaly'] = (anomaly_labels == -1)
        df_clean['anomaly_score'] = anomaly_scores
        
        # –¢–æ–ø –∞–Ω–æ–º–∞–ª–∏–π
        anomalies = df_clean[df_clean['is_anomaly']].sort_values('anomaly_score')
        
        return anomalies, {
            'total_anomalies': len(anomalies),
            'anomaly_rate': len(anomalies) / len(df_clean) * 100
        }
    
    def segment_customers(self, df, n_clusters=3):
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é K-Means"""
        
        if not SKLEARN_AVAILABLE:
            return None, "Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
            
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        segment_features = [
            'avg_order_value', 'customer_retention', 'marketing_efficiency',
            'rating', 'total_orders'
        ]
        
        df_agg = df.groupby(df.index // 7).agg({  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–µ–¥–µ–ª—è–º
            'avg_order_value': 'mean',
            'customer_retention': 'mean', 
            'marketing_efficiency': 'mean',
            'rating': 'mean',
            'total_orders': 'sum'
        }).dropna()
        
        if len(df_agg) < n_clusters:
            return None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"
            
        X = df_agg[segment_features]
        X_scaled = self.scaler.fit_transform(X)
        
        # K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)
        
        df_agg['cluster'] = clusters
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_analysis = {}
        for i in range(n_clusters):
            cluster_data = df_agg[df_agg['cluster'] == i]
            cluster_analysis[i] = {
                'size': len(cluster_data),
                'avg_order_value': cluster_data['avg_order_value'].mean(),
                'customer_retention': cluster_data['customer_retention'].mean(),
                'rating': cluster_data['rating'].mean(),
                'description': self._describe_cluster(cluster_data, segment_features)
            }
        
        return df_agg, cluster_analysis
    
    def _describe_cluster(self, cluster_data, features):
        """–û–ø–∏—Å—ã–≤–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        
        descriptions = []
        
        if cluster_data['avg_order_value'].mean() > 400000:
            descriptions.append("–ü—Ä–µ–º–∏—É–º-—Å–µ–≥–º–µ–Ω—Ç")
        elif cluster_data['avg_order_value'].mean() > 250000:
            descriptions.append("–°—Ä–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç")
        else:
            descriptions.append("–ë—é–¥–∂–µ—Ç–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç")
            
        if cluster_data['customer_retention'].mean() > 0.6:
            descriptions.append("–í—ã—Å–æ–∫–∞—è –ª–æ—è–ª—å–Ω–æ—Å—Ç—å")
        elif cluster_data['customer_retention'].mean() > 0.4:
            descriptions.append("–°—Ä–µ–¥–Ω—è—è –ª–æ—è–ª—å–Ω–æ—Å—Ç—å")
        else:
            descriptions.append("–ù–∏–∑–∫–∞—è –ª–æ—è–ª—å–Ω–æ—Å—Ç—å")
            
        if cluster_data['rating'].mean() > 4.7:
            descriptions.append("–û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ")
        elif cluster_data['rating'].mean() > 4.5:
            descriptions.append("–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ")
        else:
            descriptions.append("–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
            
        return " | ".join(descriptions)
    
    def forecast_sales(self, df, periods=30):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ —Å –ø–æ–º–æ—â—å—é Prophet"""
        
        if not PROPHET_AVAILABLE:
            return None, "Prophet –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
            
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Prophet
        prophet_df = df[['stat_date', 'total_sales']].copy()
        prophet_df.columns = ['ds', 'y']
        prophet_df = prophet_df.dropna()
        
        if len(prophet_df) < 30:
            return None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Prophet
        model = Prophet(
            daily_seasonality=True,
            weekly_seasonality=True,
            yearly_seasonality=True,
            changepoint_prior_scale=0.05
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if 'marketing_spend' in df.columns:
            model.add_regressor('marketing_spend')
            prophet_df['marketing_spend'] = df['marketing_spend'].values[:len(prophet_df)]
            
        if 'is_weekend' in df.columns:
            model.add_regressor('is_weekend')
            prophet_df['is_weekend'] = df['is_weekend'].values[:len(prophet_df)]
        
        model.fit(prophet_df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞
        future = model.make_future_dataframe(periods=periods)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –±—É–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤
        if 'marketing_spend' in prophet_df.columns:
            avg_marketing = prophet_df['marketing_spend'].tail(30).mean()
            future['marketing_spend'] = future['marketing_spend'].fillna(avg_marketing)
            
        if 'is_weekend' in prophet_df.columns:
            future['is_weekend'] = future['ds'].dt.dayofweek.isin([5, 6]).astype(int)
        
        forecast = model.predict(future)
        
        return forecast, model
    
    def generate_ml_insights(self, restaurant_id, start_date=None, end_date=None):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ML –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞"""
        
        insights = []
        insights.append("ü§ñ –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï - –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
        insights.append("=" * 60)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = self.load_restaurant_data(restaurant_id, start_date, end_date)
        
        if len(df) < 10:
            insights.append("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞")
            return insights
            
        insights.append(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        insights.append("")
        
        # 1. –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂
        insights.append("üéØ 1. –ú–û–î–ï–õ–¨ –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–†–û–î–ê–ñ")
        insights.append("-" * 40)
        
        model, metrics = self.train_sales_prediction_model(df, 'random_forest')
        if model:
            insights.append(f"‚úÖ Random Forest –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
            insights.append(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å (R¬≤): {metrics['r2']:.3f}")
            insights.append(f"üìä –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {metrics['mae']:,.0f} IDR")
            insights.append("")
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if metrics['feature_importance']:
                insights.append("üîç –¢–û–ü-5 –§–ê–ö–¢–û–†–û–í, –í–õ–ò–Ø–Æ–©–ò–• –ù–ê –ü–†–û–î–ê–ñ–ò:")
                for i, (feature, importance) in enumerate(metrics['feature_importance'][:5]):
                    feature_name = self._translate_feature(feature)
                    insights.append(f"  {i+1}. {feature_name}: {importance:.3f}")
                insights.append("")
        
        # 2. –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        insights.append("üö® 2. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –î–ï–¢–ï–ö–¶–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô")
        insights.append("-" * 40)
        
        anomalies, anomaly_stats = self.detect_anomalies(df)
        if anomalies is not None:
            insights.append(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {anomaly_stats['total_anomalies']} –∞–Ω–æ–º–∞–ª–∏–π")
            insights.append(f"üìä –î–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π: {anomaly_stats['anomaly_rate']:.1f}%")
            
            if len(anomalies) > 0:
                insights.append("")
                insights.append("üìâ –¢–û–ü-3 –ê–ù–û–ú–ê–õ–¨–ù–´–• –î–ù–Ø:")
                for i, (idx, row) in enumerate(anomalies.head(3).iterrows()):
                    date = row['stat_date'].strftime('%Y-%m-%d')
                    sales = row['total_sales']
                    score = row['anomaly_score']
                    insights.append(f"  {i+1}. {date}: {sales:,.0f} IDR (–∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç—å: {abs(score):.3f})")
            insights.append("")
        
        # 3. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–æ–≤
        insights.append("üìä 3. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø –ü–ï–†–ò–û–î–û–í")
        insights.append("-" * 40)
        
        segments, cluster_analysis = self.segment_customers(df)
        if segments is not None:
            insights.append(f"üéØ –í—ã–¥–µ–ª–µ–Ω–æ {len(cluster_analysis)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤:")
            for cluster_id, analysis in cluster_analysis.items():
                insights.append(f"  –°–µ–≥–º–µ–Ω—Ç {cluster_id + 1}: {analysis['description']}")
                insights.append(f"    ‚Ä¢ –†–∞–∑–º–µ—Ä: {analysis['size']} –ø–µ—Ä–∏–æ–¥–æ–≤")
                insights.append(f"    ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: {analysis['avg_order_value']:,.0f} IDR")
                insights.append(f"    ‚Ä¢ –õ–æ—è–ª—å–Ω–æ—Å—Ç—å: {analysis['customer_retention']:.1%}")
            insights.append("")
        
        # 4. –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂
        insights.append("üîÆ 4. –ü–†–û–ì–ù–û–ó –ü–†–û–î–ê–ñ (PROPHET)")
        insights.append("-" * 40)
        
        forecast, prophet_model = self.forecast_sales(df, periods=14)
        if forecast is not None:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏
            recent_avg = df['total_sales'].tail(7).mean()
            
            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ 7 –¥–Ω–µ–π
            future_forecast = forecast.tail(14).head(7)
            forecast_avg = future_forecast['yhat'].mean()
            
            growth_rate = (forecast_avg - recent_avg) / recent_avg * 100
            
            insights.append(f"üìà –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ 7 –¥–Ω–µ–π:")
            insights.append(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏: {forecast_avg:,.0f} IDR/–¥–µ–Ω—å")
            insights.append(f"  ‚Ä¢ –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {growth_rate:+.1f}% –∫ —Ç–µ–∫—É—â–µ–º—É –ø–µ—Ä–∏–æ–¥—É")
            
            # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            forecast_min = future_forecast['yhat_lower'].mean()
            forecast_max = future_forecast['yhat_upper'].mean()
            insights.append(f"  ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: {forecast_min:,.0f} - {forecast_max:,.0f} IDR")
            insights.append("")
        
        # 5. ML —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        insights.append("üí° 5. ML-–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        insights.append("-" * 40)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        if model and metrics['feature_importance']:
            top_factor = metrics['feature_importance'][0]
            factor_name = self._translate_feature(top_factor[0])
            insights.append(f"üéØ –ì–ª–∞–≤–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä —Ä–æ—Å—Ç–∞: {factor_name}")
            
            if 'marketing' in top_factor[0].lower():
                insights.append("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ —Ä–∞—Å—Ö–æ–¥—ã")
            elif 'rating' in top_factor[0].lower():
                insights.append("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –§–æ–∫—É—Å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è")
            elif 'operational' in top_factor[0].lower():
                insights.append("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£–ª—É—á—à–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
        
        # –ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if forecast is not None and growth_rate < -5:
            insights.append("‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂")
            insights.append("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ü—Ä–∏–Ω—è—Ç—å –ø—Ä–µ–≤–µ–Ω—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã")
        elif forecast is not None and growth_rate > 10:
            insights.append("üöÄ –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –ø—Ä–æ–¥–∞–∂")
            insights.append("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Å–ø—Ä–æ—Å–∞")
        
        return insights
    
    def _translate_feature(self, feature):
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
        
        translations = {
            'rating': '–†–µ–π—Ç–∏–Ω–≥',
            'marketing_spend': '–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ —Ä–∞—Å—Ö–æ–¥—ã',
            'marketing_efficiency': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞',
            'customer_retention': '–£–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤',
            'avg_order_value': '–°—Ä–µ–¥–Ω–∏–π —á–µ–∫',
            'operational_issues': '–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã',
            'total_orders': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤',
            'is_weekend': '–í—ã—Ö–æ–¥–Ω—ã–µ –¥–Ω–∏',
            'order_completion': '–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑–æ–≤',
            'negative_reviews': '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã',
            'positive_reviews': '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã',
            'sales_lag_1': '–ü—Ä–æ–¥–∞–∂–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–Ω—è',
            'sales_lag_7': '–ü—Ä–æ–¥–∞–∂–∏ –Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥'
        }
        
        return translations.get(feature, feature)

def analyze_restaurant_with_ml(restaurant_name, start_date=None, end_date=None):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ML –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞"""
    
    # –ü–æ–ª—É—á–∞–µ–º ID —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞
    conn = sqlite3.connect('database.sqlite')
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM restaurants WHERE name = ?", (restaurant_name,))
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        return ["‚ùå –†–µ—Å—Ç–æ—Ä–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω"]
    
    restaurant_id = result[0]
    
    # –°–æ–∑–¥–∞–µ–º ML –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    ml_analyzer = RestaurantMLAnalyzer()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ML –∏–Ω—Å–∞–π—Ç—ã
    ml_insights = ml_analyzer.generate_ml_insights(
        restaurant_id, start_date, end_date
    )
    
    return ml_insights

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –º–æ–¥—É–ª–µ–π
    print("ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –º–æ–¥—É–ª–µ–π...")
    
    if SKLEARN_AVAILABLE:
        print("‚úÖ Scikit-learn –¥–æ—Å—Ç—É–ø–µ–Ω")
    else:
        print("‚ùå Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
    if PROPHET_AVAILABLE:
        print("‚úÖ Prophet –¥–æ—Å—Ç—É–ø–µ–Ω")
    else:
        print("‚ùå Prophet –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")