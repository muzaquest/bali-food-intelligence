#!/usr/bin/env python3
"""
üîç ML-POWERED –î–ï–¢–ï–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ù–ê–°–¢–û–Ø–©–ò–ô –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω –ø–∞–¥–µ–Ω–∏–π –∏ —Ä–æ—Å—Ç–∞ –ø—Ä–æ–¥–∞–∂
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Random Forest + SHAP –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏–π
"""

import pandas as pd
import numpy as np
import sqlite3
import json
from datetime import datetime, timedelta
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import shap

class MLDetectiveAnalyzer:
    """ML-powered –¥–µ—Ç–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –∞–Ω–æ–º–∞–ª–∏–π –ø—Ä–æ–¥–∞–∂"""
    
    def __init__(self, db_path='database.sqlite'):
        self.db_path = db_path
        self.model = None
        self.explainer = None
        self.feature_names = []
        self.scaler = StandardScaler()
        
    def load_comprehensive_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–ï–°–¨ –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö 59 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤"""
        
        print("üî¨ –ó–ê–ì–†–£–ñ–ê–ï–ú –ö–û–ú–ü–õ–ï–ö–°–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –í–°–ï–• 59 –†–ï–°–¢–û–†–ê–ù–û–í...")
        
        conn = sqlite3.connect(self.db_path)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤
        query = """
        SELECT 
            -- –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            g.stat_date,
            r.name as restaurant_name,
            COALESCE(g.sales, 0) + COALESCE(gj.sales, 0) as total_sales,
            COALESCE(g.orders, 0) + COALESCE(gj.orders, 0) as total_orders,
            COALESCE(g.ads_spend, 0) + COALESCE(gj.ads_spend, 0) as marketing_spend,
            
            -- –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            COALESCE(g.new_customers, 0) + COALESCE(gj.new_client, 0) as new_customers,
            COALESCE(g.repeated_customers, 0) + COALESCE(gj.returned_client, 0) as returning_customers,
            
            -- –†–µ–π—Ç–∏–Ω–≥–∏
            CASE 
                WHEN g.rating > 0 AND gj.rating > 0 THEN (g.rating + gj.rating) / 2
                WHEN g.rating > 0 THEN g.rating
                WHEN gj.rating > 0 THEN gj.rating
                ELSE 4.5
            END as rating,
            
            -- –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            COALESCE(g.cancelled_orders, 0) + COALESCE(gj.cancelled_orders, 0) as cancelled_orders,
            COALESCE(g.ads_orders, 0) + COALESCE(gj.ads_orders, 0) as promo_orders,
            
            -- –ö–æ–Ω–≤–µ—Ä—Å–∏—è (—Ç–æ–ª—å–∫–æ Grab –∏–º–µ–µ—Ç –¥–∞–Ω–Ω—ã–µ)
            COALESCE(g.impressions, 0) as ad_impressions,
            COALESCE(g.unique_menu_visits, 0) as menu_views,
            COALESCE(g.unique_add_to_carts, 0) as add_to_cart,
            COALESCE(g.unique_conversion_reach, 0) as conversions,
            
            -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            CAST(strftime('%w', g.stat_date) AS INTEGER) as day_of_week,
            CAST(strftime('%m', g.stat_date) AS INTEGER) as month,
            CAST(strftime('%d', g.stat_date) AS INTEGER) as day_of_month,
            
            -- –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—á–µ—Ä–∞—à–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏)
            LAG(COALESCE(g.sales, 0) + COALESCE(gj.sales, 0), 1) OVER (
                PARTITION BY r.name ORDER BY g.stat_date
            ) as prev_day_sales,
            
            -- 7-–¥–Ω–µ–≤–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
            AVG(COALESCE(g.sales, 0) + COALESCE(gj.sales, 0)) OVER (
                PARTITION BY r.name 
                ORDER BY g.stat_date 
                ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
            ) as sales_7day_avg
            
        FROM grab_stats g
        LEFT JOIN restaurants r ON g.restaurant_id = r.id
        LEFT JOIN gojek_stats gj ON g.restaurant_id = gj.restaurant_id 
                                 AND g.stat_date = gj.stat_date
        WHERE g.stat_date >= '2024-01-01' AND r.name IS NOT NULL
        ORDER BY r.name, g.stat_date
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {df['restaurant_name'].nunique()} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        df = self.add_external_factors(df)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.fillna(0)
        
        return df
    
    def add_external_factors(self, df):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã: –ø–æ–≥–æ–¥–∞, —Ç—É—Ä–∏—Å—Ç—ã, –ø—Ä–∞–∑–¥–Ω–∏–∫–∏, –ª–æ–∫–∞—Ü–∏—è"""
        
        print("üåç –î–û–ë–ê–í–õ–Ø–ï–ú –í–ù–ï–®–ù–ò–ï –§–ê–ö–¢–û–†–´...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
        try:
            with open('combined_tourist_correlations_2024_2025.json', 'r', encoding='utf-8') as f:
                tourist_data = json.load(f)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Å—è—á–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
            tourist_coeffs = {}
            for month_str, data in tourist_data['monthly_coefficients'].items():
                tourist_coeffs[int(month_str)] = data['coefficient']
                
        except:
            # –§–æ–ª–±—ç–∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            tourist_coeffs = {i: 1.0 for i in range(1, 13)}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        df['tourist_coefficient'] = df['month'].map(tourist_coeffs).fillna(1.0)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        try:
            with open('real_coefficients.json', 'r', encoding='utf-8') as f:
                weather_data = json.load(f)
            weather_impact = weather_data.get('weather', {}).get('rain_impact', -0.15)
        except:
            weather_impact = -0.15
            
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ –¥–Ω–∏ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥–∫–ª—é—á–∏–ª–∏ –±—ã Weather API)
        np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        df['is_rainy'] = np.random.choice([0, 1], size=len(df), p=[0.7, 0.3])
        df['weather_impact'] = df['is_rainy'] * weather_impact
        
        # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        holidays = [
            '2024-01-01', '2024-02-10', '2024-03-11', '2024-04-21', 
            '2024-05-01', '2024-06-06', '2024-08-17', '2024-12-25'
        ]
        df['is_holiday'] = df['stat_date'].isin(holidays).astype(int)
        df['holiday_impact'] = df['is_holiday'] * 0.1  # +10% –≤ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏
        
        # –õ–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞–º)
        location_factors = {
            'Seminyak': 1.2, 'Canggu': 1.15, 'Ubud': 1.1, 'Kuta': 1.05,
            'Sanur': 1.0, 'Jimbaran': 1.0, 'Nusa Dua': 1.1, 'Denpasar': 0.95
        }
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞
        def get_location_factor(restaurant_name):
            for location, factor in location_factors.items():
                if location.lower() in restaurant_name.lower():
                    return factor
            return 1.0
        
        df['location_factor'] = df['restaurant_name'].apply(get_location_factor)
        
        # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ (—Å–∏–º—É–ª—è—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –≤ —Ä–∞–π–æ–Ω–µ)
        restaurant_counts = df.groupby('stat_date')['restaurant_name'].nunique()
        df['competition_pressure'] = df['stat_date'].map(restaurant_counts) / 59.0
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã: —Ç—É—Ä–∏—Å—Ç—ã, –ø–æ–≥–æ–¥–∞, –ø—Ä–∞–∑–¥–Ω–∏–∫–∏, –ª–æ–∫–∞—Ü–∏—è, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è")
        
        return df
    
    def prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML –º–æ–¥–µ–ª–∏"""
        
        # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        feature_columns = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
            'total_orders', 'marketing_spend', 'new_customers', 'returning_customers',
            'rating', 'cancelled_orders', 'promo_orders',
            
            # –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            'ad_impressions', 'menu_views', 'add_to_cart', 'conversions',
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'day_of_week', 'month', 'day_of_month',
            
            # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'prev_day_sales', 'sales_7day_avg',
            
            # –í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
            'tourist_coefficient', 'weather_impact', 'holiday_impact', 
            'location_factor', 'competition_pressure'
        ]
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['avg_order_value'] = np.where(df['total_orders'] > 0, 
                                        df['total_sales'] / df['total_orders'], 0)
        df['marketing_efficiency'] = np.where(df['marketing_spend'] > 0,
                                            df['total_sales'] / df['marketing_spend'], 0)
        df['customer_retention_rate'] = np.where(
            (df['new_customers'] + df['returning_customers']) > 0,
            df['returning_customers'] / (df['new_customers'] + df['returning_customers']), 0
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–ø–∏—Å–æ–∫
        feature_columns.extend(['avg_order_value', 'marketing_efficiency', 'customer_retention_rate'])
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['marketing_x_tourist'] = df['marketing_spend'] * df['tourist_coefficient']
        df['weather_x_orders'] = df['weather_impact'] * df['total_orders']
        df['weekend_effect'] = ((df['day_of_week'] == 0) | (df['day_of_week'] == 6)).astype(int)
        
        feature_columns.extend(['marketing_x_tourist', 'weather_x_orders', 'weekend_effect'])
        
        # –£–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        df = df.dropna(subset=feature_columns + ['total_sales'])
        
        self.feature_names = feature_columns
        
        return df[feature_columns + ['total_sales', 'stat_date', 'restaurant_name']]
    
    def train_model(self, df):
        """–û–±—É—á–∞–µ—Ç Random Forest –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print("ü§ñ –û–ë–£–ß–ê–ï–ú RANDOM FOREST –ù–ê –í–°–ï–• –î–ê–ù–ù–´–•...")
        
        X = df[self.feature_names]
        y = df['total_sales']
        
        print(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X)} –∑–∞–ø–∏—Å–µ–π, {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=True
        )
        
        # –û–±—É—á–∞–µ–º Random Forest
        self.model = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        y_pred = self.model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞! MAE: {mae:,.0f} IDR, R¬≤: {r2:.3f}")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = dict(zip(self.feature_names, self.model.feature_importances_))
        feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print("\nüèÜ –¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
        for feature, importance in feature_importance[:10]:
            print(f"   {feature}: {importance:.3f}")
        
        # –°–æ–∑–¥–∞–µ–º SHAP explainer
        print("üîç –°–û–ó–î–ê–ï–ú SHAP EXPLAINER...")
        self.explainer = shap.TreeExplainer(self.model)
        
        return {
            'mae': mae,
            'r2': r2,
            'feature_importance': feature_importance
        }
    
    def explain_anomaly(self, restaurant_name, date, df):
        """–û–±—ä—è—Å–Ω—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∞–Ω–æ–º–∞–ª–∏—é —Å –ø–æ–º–æ—â—å—é SHAP"""
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        record = df[(df['restaurant_name'] == restaurant_name) & 
                   (df['stat_date'] == date)]
        
        if record.empty:
            return None
            
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏
        X_record = record[self.feature_names].values
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        predicted_sales = self.model.predict(X_record)[0]
        actual_sales = record['total_sales'].iloc[0]
        
        # SHAP –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        shap_values = self.explainer.shap_values(X_record)
        
        # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
        base_value = self.explainer.expected_value
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–ª–∏—è–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞
        influences = {}
        for i, feature in enumerate(self.feature_names):
            shap_contribution = shap_values[0][i]
            feature_value = X_record[0][i]
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º SHAP –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ
            if base_value != 0:
                influence_percent = (shap_contribution / base_value) * 100
            else:
                influence_percent = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º numpy –º–∞—Å—Å–∏–≤—ã
            if hasattr(influence_percent, '__iter__') and not isinstance(influence_percent, str):
                influence_percent = float(influence_percent[0]) if len(influence_percent) > 0 else 0.0
            if hasattr(feature_value, '__iter__') and not isinstance(feature_value, str):
                feature_value = float(feature_value[0]) if len(feature_value) > 0 else 0.0
                
            influences[feature] = {
                'shap_value': float(shap_contribution),
                'feature_value': float(feature_value),
                'influence_percent': float(influence_percent)
            }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –≤–ª–∏—è–Ω–∏—é
        sorted_influences = sorted(influences.items(), 
                                 key=lambda x: abs(x[1]['influence_percent']), 
                                 reverse=True)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ
        total_explained = sum(abs(inf['influence_percent']) for _, inf in influences.items())
        actual_deviation = ((actual_sales - predicted_sales) / predicted_sales * 100) if predicted_sales > 0 else 0
        unexplained = actual_deviation - sum(inf['influence_percent'] for _, inf in influences.items())
        
        return {
            'actual_sales': actual_sales,
            'predicted_sales': predicted_sales,
            'deviation_percent': actual_deviation,
            'base_value': base_value,
            'influences': sorted_influences[:10],  # –¢–æ–ø-10 –≤–ª–∏—è–Ω–∏–π
            'unexplained_percent': unexplained,
            'total_explained_percent': total_explained
        }
    
    def analyze_restaurant_anomalies(self, restaurant_name, start_date, end_date, df):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∞–Ω–æ–º–∞–ª–∏–∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ —Å ML –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏"""
        
        print(f"üîç ML-–ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô: {restaurant_name}")
        print("=" * 60)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞
        restaurant_data = df[
            (df['restaurant_name'] == restaurant_name) &
            (df['stat_date'] >= start_date) &
            (df['stat_date'] <= end_date)
        ].copy()
        
        if restaurant_data.empty:
            return []
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        mean_sales = restaurant_data['total_sales'].mean()
        std_sales = restaurant_data['total_sales'].std()
        
        # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–æ–º–∞–ª–∏–∏ (–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è > 15% –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ)
        anomalies = []
        
        for _, row in restaurant_data.iterrows():
            deviation_percent = ((row['total_sales'] - mean_sales) / mean_sales) * 100
            
            if abs(deviation_percent) > 15:  # –ê–Ω–æ–º–∞–ª–∏—è –µ—Å–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ > 15%
                
                # ML –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏
                explanation = self.explain_anomaly(restaurant_name, row['stat_date'], df)
                
                if explanation:
                    anomaly_type = "üü¢ üìà –†–û–°–¢" if deviation_percent > 0 else "üî¥ üìâ –ü–ê–î–ï–ù–ò–ï"
                    
                    anomalies.append({
                        'date': row['stat_date'],
                        'type': anomaly_type,
                        'actual_sales': row['total_sales'],
                        'mean_sales': mean_sales,
                        'deviation_percent': deviation_percent,
                        'ml_explanation': explanation
                    })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        anomalies = sorted(anomalies, key=lambda x: x['date'])
        
        print(f"üö® –ù–ê–ô–î–ï–ù–û {len(anomalies)} ML-–û–ë–™–Ø–°–ù–ï–ù–ù–´–• –ê–ù–û–ú–ê–õ–ò–ô")
        
        return anomalies
    
    def format_ml_detective_report(self, anomalies):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç ML –¥–µ—Ç–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç—á–µ—Ç"""
        
        report = []
        report.append("üîç ML-POWERED –î–ï–¢–ï–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ò–ß–ò–ù")
        report.append("=" * 60)
        report.append(f"ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω Random Forest + SHAP –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω")
        report.append(f"üìä –û–ë–ù–ê–†–£–ñ–ï–ù–û {len(anomalies)} ML-–û–ë–™–Ø–°–ù–ï–ù–ù–´–• –ê–ù–û–ú–ê–õ–ò–ô:")
        report.append("")
        
        for i, anomaly in enumerate(anomalies, 1):
            explanation = anomaly['ml_explanation']
            
            report.append(f"{i:2d}. {anomaly['date']}: {anomaly['type']} –Ω–∞ {anomaly['deviation_percent']:+.1f}%")
            report.append(f"    üí∞ –ü—Ä–æ–¥–∞–∂–∏: {anomaly['actual_sales']:,.0f} IDR (—Å—Ä–µ–¥–Ω–µ–µ: {anomaly['mean_sales']:,.0f} IDR)")
            report.append(f"    ü§ñ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {explanation['predicted_sales']:,.0f} IDR")
            report.append("    üîç ML-–û–ë–™–Ø–°–ù–ï–ù–ò–ï –ü–†–ò–ß–ò–ù:")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 –≤–ª–∏—è–Ω–∏–π
            for feature, influence in explanation['influences'][:5]:
                influence_pct = influence['influence_percent']
                feature_val = influence['feature_value']
                
                if abs(influence_pct) > 1:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –≤–ª–∏—è–Ω–∏—è
                    emoji = "üìà" if influence_pct > 0 else "üìâ"
                    report.append(f"       ‚Ä¢ {emoji} {feature}: {influence_pct:+.1f}% (–∑–Ω–∞—á–µ–Ω–∏–µ: {feature_val:.2f})")
            
            # –ù–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ
            unexplained = explanation['unexplained_percent']
            if abs(unexplained) > 5:
                report.append(f"       ‚Ä¢ ‚ùì –ù–ï–û–ë–™–Ø–°–ù–ï–ù–ù–û–ï ML-–í–õ–ò–Ø–ù–ò–ï: {unexplained:+.1f}% (—Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)")
            
            report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–º—É –≤–ª–∏—è–Ω–∏—é
        unexplained_values = [abs(a['ml_explanation']['unexplained_percent']) for a in anomalies]
        if unexplained_values:
            avg_unexplained = np.mean(unexplained_values)
            max_unexplained = max(unexplained_values)
            
            report.append("üìä ML-–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ù–ï–û–ë–™–Ø–°–ù–ï–ù–ù–û–ì–û –í–õ–ò–Ø–ù–ò–Ø:")
            report.append(f"   üìà –°—Ä–µ–¥–Ω–µ–µ –Ω–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–µ: {avg_unexplained:.1f}%")
            report.append(f"   üî¥ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –Ω–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–µ: {max_unexplained:.1f}%")
            
            if avg_unexplained < 15:
                report.append("   ‚úÖ –û–¢–õ–ò–ß–ù–û: ML –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–∏—á–∏–Ω—ã!")
            elif avg_unexplained < 25:
                report.append("   üü° –•–û–†–û–®–û: ML –º–æ–¥–µ–ª—å –æ–±—ä—è—Å–Ω—è–µ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–∏—á–∏–Ω")
            else:
                report.append("   üî¥ –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø: –ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ML")
        
        return "\n".join(report)

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –¥–µ—Ç–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    print("üöÄ –ó–ê–ü–£–°–ö ML-POWERED –î–ï–¢–ï–ö–¢–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = MLDetectiveAnalyzer()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    df = analyzer.load_comprehensive_data()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    df = analyzer.prepare_features(df)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    metrics = analyzer.train_model(df)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ Ika Canggu
    anomalies = analyzer.analyze_restaurant_anomalies(
        'Ika Canggu', '2025-04-01', '2025-06-30', df
    )
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = analyzer.format_ml_detective_report(anomalies)
    print("\n" + report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open('ml_detective_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'model_metrics': metrics,
            'anomalies_count': len(anomalies),
            'feature_names': analyzer.feature_names,
            'analysis_date': datetime.now().isoformat()
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ ML –¥–µ—Ç–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìä –ú–æ–¥–µ–ª—å R¬≤: {metrics['r2']:.3f}")
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomalies)}")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ml_detective_results.json")

if __name__ == "__main__":
    main()