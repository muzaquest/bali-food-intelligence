#!/usr/bin/env python3
"""
üöÄ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø ML –°–ò–°–¢–ï–ú–ê –ü–û –ü–õ–ê–ù–£ CHATGPT
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥ –Ω–∞—à—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö: grab_stats + gojek_stats + restaurants
"""

import pandas as pd
import numpy as np
import sqlite3
import json
import requests
from datetime import datetime, timedelta
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
import lightgbm as lgb
import shap
import joblib
import warnings
warnings.filterwarnings('ignore')

class ProfessionalMLSystem:
    """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è ML —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–¥–∞–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤"""
    
    def __init__(self):
        self.models = []
        self.feature_names = []
        self.weather_cache = {}
        self.shap_explainer = None
        self.feature_importance = None
        
    def load_and_validate_data(self):
        """1) –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö (ChatGPT —á–µ–∫–ª–∏—Å—Ç)"""
        
        print("üîç –≠–¢–ê–ü 1: –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–•")
        print("="*50)
        
        conn = sqlite3.connect('database.sqlite')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö
        validation_results = {}
        
        # 1.1 –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        tables_check = pd.read_sql_query("""
            SELECT 
                'grab_stats' as table_name,
                COUNT(*) as records,
                COUNT(DISTINCT restaurant_id) as restaurants,
                COUNT(DISTINCT stat_date) as dates,
                MIN(stat_date) as min_date,
                MAX(stat_date) as max_date
            FROM grab_stats
            WHERE sales > 0
            
            UNION ALL
            
            SELECT 
                'gojek_stats' as table_name,
                COUNT(*) as records,
                COUNT(DISTINCT restaurant_id) as restaurants,
                COUNT(DISTINCT stat_date) as dates,
                MIN(stat_date) as min_date,
                MAX(stat_date) as max_date
            FROM gojek_stats
            WHERE sales > 0
        """, conn)
        
        print("üìä –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        for _, row in tables_check.iterrows():
            print(f"   {row['table_name']:12}: {row['records']:,} –∑–∞–ø–∏—Å–µ–π, {row['restaurants']} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
            print(f"                     –ü–µ—Ä–∏–æ–¥: {row['min_date']} ‚Üí {row['max_date']}")
        
        # 1.2 –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤
        coords_check = pd.read_sql_query("""
            SELECT 
                COUNT(*) as total_restaurants,
                COUNT(latitude) as with_coords,
                COUNT(CASE WHEN latitude IS NULL THEN 1 END) as missing_coords
            FROM restaurants
        """, conn)
        
        total_rest = coords_check.iloc[0]['total_restaurants']
        with_coords = coords_check.iloc[0]['with_coords']
        missing_coords = coords_check.iloc[0]['missing_coords']
        
        print(f"\nüó∫Ô∏è –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤:")
        print(f"   –í—Å–µ–≥–æ: {total_rest}, –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords} ({with_coords/total_rest*100:.1f}%)")
        
        validation_results['coords_coverage'] = with_coords/total_rest
        
        # 1.3 –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        data_quality = pd.read_sql_query("""
            SELECT 
                COUNT(CASE WHEN g.sales IS NULL THEN 1 END) as grab_null_sales,
                COUNT(CASE WHEN gj.sales IS NULL THEN 1 END) as gojek_null_sales,
                COUNT(CASE WHEN g.orders IS NULL THEN 1 END) as grab_null_orders,
                COUNT(CASE WHEN gj.orders IS NULL THEN 1 END) as gojek_null_orders
            FROM grab_stats g
            FULL OUTER JOIN gojek_stats gj ON g.restaurant_id = gj.restaurant_id AND g.stat_date = gj.stat_date
        """, conn)
        
        print(f"\nüîç –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   GRAB –ø—Ä–æ–ø—É—Å–∫–∏: sales={data_quality.iloc[0]['grab_null_sales']}, orders={data_quality.iloc[0]['grab_null_orders']}")
        print(f"   GOJEK –ø—Ä–æ–ø—É—Å–∫–∏: sales={data_quality.iloc[0]['gojek_null_sales']}, orders={data_quality.iloc[0]['gojek_null_orders']}")
        
        conn.close()
        
        if validation_results['coords_coverage'] >= 0.9:
            print("‚úÖ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ü–†–û–ô–î–ï–ù–´")
            return True
        else:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
            return False
    
    def build_feature_dataset(self):
        """2) Feature Engineering - —Å–æ–∑–¥–∞–Ω–∏–µ –±–æ–≥–∞—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ñ–∏—á–µ–π"""
        
        print("\nüîß –≠–¢–ê–ü 2: FEATURE ENGINEERING")
        print("="*40)
        
        conn = sqlite3.connect('database.sqlite')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è GRAB + GOJEK –¥–∞–Ω–Ω—ã—Ö
        main_query = """
        SELECT 
            COALESCE(g.restaurant_id, gj.restaurant_id) as restaurant_id,
            COALESCE(g.stat_date, gj.stat_date) as stat_date,
            
            -- GRAB –¥–∞–Ω–Ω—ã–µ
            COALESCE(g.sales, 0) as grab_sales,
            COALESCE(g.orders, 0) as grab_orders,
            COALESCE(g.ads_sales, 0) as grab_ads_sales,
            COALESCE(g.ads_spend, 0) as grab_ads_spend,
            COALESCE(g.rating, 0) as grab_rating,
            COALESCE(g.offline_rate, 0) as grab_offline_rate,
            COALESCE(g.cancelation_rate, 0) as grab_cancelation_rate,
            COALESCE(g.impressions, 0) as grab_impressions,
            COALESCE(g.ads_ctr, 0) as grab_ads_ctr,
            COALESCE(g.new_customers, 0) as grab_new_customers,
            COALESCE(g.repeated_customers, 0) as grab_repeated_customers,
            
            -- GOJEK –¥–∞–Ω–Ω—ã–µ
            COALESCE(gj.sales, 0) as gojek_sales,
            COALESCE(gj.orders, 0) as gojek_orders,
            COALESCE(gj.ads_sales, 0) as gojek_ads_sales,
            COALESCE(gj.ads_spend, 0) as gojek_ads_spend,
            COALESCE(gj.rating, 0) as gojek_rating,
            COALESCE(gj.accepting_time, '00:00:00') as gojek_accepting_time,
            COALESCE(gj.preparation_time, '00:00:00') as gojek_preparation_time,
            COALESCE(gj.delivery_time, '00:00:00') as gojek_delivery_time,
            COALESCE(gj.lost_orders, 0) as gojek_lost_orders,
            COALESCE(gj.realized_orders_percentage, 100) as gojek_realized_pct,
            COALESCE(gj.close_time, 0) as gojek_close_time,
            
            -- –†–µ—Å—Ç–æ—Ä–∞–Ω –¥–∞–Ω–Ω—ã–µ
            r.latitude,
            r.longitude,
            r.location_region
            
        FROM grab_stats g
        FULL OUTER JOIN gojek_stats gj ON g.restaurant_id = gj.restaurant_id AND g.stat_date = gj.stat_date
        LEFT JOIN restaurants r ON COALESCE(g.restaurant_id, gj.restaurant_id) = r.id
        WHERE (g.sales > 0 OR gj.sales > 0)
        ORDER BY stat_date, restaurant_id
        """
        
        df = pd.read_sql_query(main_query, conn)
        conn.close()
        
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö (–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï)
        print("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        
        numeric_cols = [
            'grab_sales', 'grab_orders', 'grab_ads_sales', 'grab_ads_spend', 'grab_rating',
            'gojek_sales', 'gojek_orders', 'gojek_ads_sales', 'gojek_ads_spend', 'gojek_rating',
            'grab_offline_rate', 'grab_cancelation_rate', 'grab_impressions', 'grab_ads_ctr',
            'grab_new_customers', 'grab_repeated_customers', 'gojek_lost_orders',
            'gojek_realized_pct', 'gojek_close_time'
        ]
        
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        # –°–æ–∑–¥–∞–µ–º TARGET –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        df['total_sales'] = df['grab_sales'] + df['gojek_sales']
        df['total_orders'] = df['grab_orders'] + df['gojek_orders']
        
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π...")
        
        # A) –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        df['stat_date'] = pd.to_datetime(df['stat_date'])
        df['day_of_week'] = df['stat_date'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['day_of_month'] = df['stat_date'].dt.day
        df['week_of_year'] = df['stat_date'].dt.isocalendar().week
        df['month'] = df['stat_date'].dt.month
        df['year'] = df['stat_date'].dt.year
        
        # B) –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        print("‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º TIME –≤ –º–∏–Ω—É—Ç—ã
        def time_to_minutes(time_str):
            if pd.isna(time_str) or time_str == '00:00:00':
                return 0
            try:
                if isinstance(time_str, str):
                    parts = time_str.split(':')
                    return int(parts[0]) * 60 + int(parts[1])
                return 0
            except:
                return 0
        
        df['gojek_accepting_minutes'] = df['gojek_accepting_time'].apply(time_to_minutes)
        df['gojek_preparation_minutes'] = df['gojek_preparation_time'].apply(time_to_minutes)
        df['gojek_delivery_minutes'] = df['gojek_delivery_time'].apply(time_to_minutes)
        
        # C) –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ —Ñ–∏—á–∏
        print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö —Ñ–∏—á–µ–π...")
        
        df['total_ads_spend'] = df['grab_ads_spend'] + df['gojek_ads_spend']
        df['total_ads_sales'] = df['grab_ads_sales'] + df['gojek_ads_sales']
        
        # ROAS calculation (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
        df['grab_roas'] = np.where(df['grab_ads_spend'] > 0, 
                                  df['grab_ads_sales'] / df['grab_ads_spend'], 0)
        df['gojek_roas'] = np.where(df['gojek_ads_spend'] > 0, 
                                   df['gojek_ads_sales'] / df['gojek_ads_spend'], 0)
        
        # D) –ü–æ–≥–æ–¥–Ω—ã–µ —Ñ–∏—á–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É)
        print("üå§Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        self._add_weather_features(df)
        
        # E) –õ–∞–≥–∏ –∏ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤ –∏ –∞–≥—Ä–µ–≥–∞—Ç–æ–≤...")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ª–∞–≥–æ–≤
        df = df.sort_values(['restaurant_id', 'stat_date'])
        
        # –õ–∞–≥–∏ –ø—Ä–æ–¥–∞–∂
        df['sales_lag_1'] = df.groupby('restaurant_id')['total_sales'].shift(1)
        df['sales_lag_7'] = df.groupby('restaurant_id')['total_sales'].shift(7)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ –∞–≥—Ä–µ–≥–∞—Ç—ã (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)
        df['sales_rolling_7'] = df.groupby('restaurant_id')['total_sales'].transform(
            lambda x: x.rolling(7, min_periods=1).mean())
        df['sales_rolling_30'] = df.groupby('restaurant_id')['total_sales'].transform(
            lambda x: x.rolling(30, min_periods=1).mean())
        
        # F) –ë–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∂–¥—è (ChatGPT —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è)
        df['rain_category'] = pd.cut(df['weather_rain'], 
                                   bins=[0, 0.1, 5, 20, 1000], 
                                   labels=['none', 'light', 'moderate', 'heavy'],
                                   include_lowest=True)
        
        # One-hot encoding –¥–ª—è –¥–æ–∂–¥—è
        rain_dummies = pd.get_dummies(df['rain_category'], prefix='rain')
        df = pd.concat([df, rain_dummies], axis=1)
        
        # G) –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏
        df['region_canggu'] = (df['location_region'] == 'canggu').astype(int)
        df['region_ubud'] = (df['location_region'] == 'ubud').astype(int)
        df['region_seminyak'] = (df['location_region'] == 'seminyak').astype(int)
        
        print(f"‚úÖ Feature Engineering –∑–∞–≤–µ—Ä—à–µ–Ω: {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        return df
    
    def _add_weather_features(self, df):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–≥–æ–¥–Ω—ã–µ —Ñ–∏—á–∏ –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—à—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É"""
        
        df['weather_temp'] = 27.0  # default
        df['weather_rain'] = 0.0
        df['weather_wind'] = 5.0
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ + —Ä–µ—Å—Ç–æ—Ä–∞–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        unique_combinations = df[['stat_date', 'restaurant_id', 'latitude', 'longitude']].drop_duplicates()
        
        # –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø: —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 500 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        max_weather_requests = min(500, len(unique_combinations))
        unique_combinations = unique_combinations.head(max_weather_requests)
        
        print(f"      –ü–æ–ª—É—á–∞–µ–º –ø–æ–≥–æ–¥—É –¥–ª—è {len(unique_combinations)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π (—Ç–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º)...")
        
        for i, (_, row) in enumerate(unique_combinations.iterrows()):
            if i % 50 == 0 and i > 0:
                print(f"      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(unique_combinations)}")
            
            date_str = row['stat_date'].strftime('%Y-%m-%d')
            restaurant_id = row['restaurant_id']
            lat, lng = row['latitude'], row['longitude']
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–≥–æ–¥—É
            weather = self._get_weather_for_date(date_str, restaurant_id, lat, lng)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —ç—Ç–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
            mask = (df['stat_date'] == row['stat_date']) & (df['restaurant_id'] == restaurant_id)
            df.loc[mask, 'weather_temp'] = weather['temp']
            df.loc[mask, 'weather_rain'] = weather['rain']
            df.loc[mask, 'weather_wind'] = weather['wind']
    
    def _get_weather_for_date(self, date, restaurant_id, lat, lng):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –∏ –¥–∞—Ç—ã"""
        
        cache_key = f"{date}_{restaurant_id}"
        
        if cache_key in self.weather_cache:
            return self.weather_cache[cache_key]
        
        default_weather = {'temp': 27.0, 'rain': 0.0, 'wind': 5.0}
        
        if pd.isna(lat) or pd.isna(lng):
            lat, lng = -8.6500, 115.2200  # Default Denpasar
        
        try:
            url = "https://archive-api.open-meteo.com/v1/archive"
            params = {
                'latitude': lat,
                'longitude': lng,
                'start_date': date,
                'end_date': date,
                'daily': 'temperature_2m_mean,precipitation_sum,wind_speed_10m_max',
                'timezone': 'Asia/Jakarta',
                'elevation': 0  # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï
            }
            
            response = requests.get(url, params=params, timeout=5)
            if response.status_code == 200:
                data = response.json()
                daily = data.get('daily', {})
                
                if daily:
                    temp_mean = daily.get('temperature_2m_mean', [None])[0]
                    precipitation = daily.get('precipitation_sum', [None])[0]
                    wind_speed = daily.get('wind_speed_10m_max', [None])[0]
                    
                    weather_data = {
                        'temp': temp_mean if temp_mean is not None else 27.0,
                        'rain': precipitation if precipitation is not None else 0.0,
                        'wind': wind_speed if wind_speed is not None else 5.0
                    }
                    
                    self.weather_cache[cache_key] = weather_data
                    return weather_data
        except:
            pass
        
        self.weather_cache[cache_key] = default_weather
        return default_weather
    
    def train_with_time_series_cv(self, df):
        """3) –û–±—É—á–µ–Ω–∏–µ —Å TimeSeriesSplit –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        
        print("\nü§ñ –≠–¢–ê–ü 3: –û–ë–£–ß–ï–ù–ò–ï –° TIME SERIES –í–ê–õ–ò–î–ê–¶–ò–ï–ô")
        print("="*50)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        target = 'total_sales'
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-—Ñ–∏—á–∏ –∏–∑ –æ–±—É—á–µ–Ω–∏—è
        exclude_cols = [
            'restaurant_id', 'stat_date', 'total_sales', 'total_orders',
            'latitude', 'longitude', 'location_region', 'rain_category',
            'gojek_accepting_time', 'gojek_preparation_time', 'gojek_delivery_time'
        ]
        
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # –£–±–∏—Ä–∞–µ–º NaN
        df_clean = df.dropna(subset=feature_cols + [target])
        
        X = df_clean[feature_cols]
        y = df_clean[target]
        
        print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   –ó–∞–ø–∏—Å–µ–π: {len(X):,}")
        print(f"   –§–∏—á–µ–π: {len(feature_cols)}")
        print(f"   Target: {target}")
        
        # TimeSeriesSplit
        tscv = TimeSeriesSplit(n_splits=5)
        
        oof_predictions = np.zeros(len(y))
        models = []
        cv_scores = []
        
        print(f"\nüîÑ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:")
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            print(f"   Fold {fold + 1}/5...")
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # –î–∞—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            train_dates = df_clean.iloc[train_idx]['stat_date']
            val_dates = df_clean.iloc[val_idx]['stat_date']
            print(f"      Train: {train_dates.min().date()} ‚Üí {train_dates.max().date()}")
            print(f"      Val:   {val_dates.min().date()} ‚Üí {val_dates.max().date()}")
            
            # LightGBM
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            
            params = {
                'objective': 'regression',
                'metric': 'l1',
                'boosting_type': 'gbdt',
                'learning_rate': 0.05,
                'num_leaves': 64,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'seed': 42,
                'verbose': -1
            }
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=2000,
                valid_sets=[val_data],
                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            val_pred = model.predict(X_val)
            oof_predictions[val_idx] = val_pred
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            mae = mean_absolute_error(y_val, val_pred)
            r2 = r2_score(y_val, val_pred)
            mape = mean_absolute_percentage_error(y_val, val_pred)
            
            cv_scores.append({'mae': mae, 'r2': r2, 'mape': mape})
            models.append(model)
            
            print(f"      MAE: {mae:,.0f} IDR, R¬≤: {r2:.4f}, MAPE: {mape:.2%}")
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        overall_mae = mean_absolute_error(y, oof_predictions)
        overall_r2 = r2_score(y, oof_predictions)
        overall_mape = mean_absolute_percentage_error(y, oof_predictions)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   MAE: {overall_mae:,.0f} IDR")
        print(f"   R¬≤: {overall_r2:.4f}")
        print(f"   MAPE: {overall_mape:.2%}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.models = models
        self.feature_names = feature_cols
        
        return {
            'models': models,
            'feature_names': feature_cols,
            'cv_scores': cv_scores,
            'overall_mae': overall_mae,
            'overall_r2': overall_r2,
            'oof_predictions': oof_predictions,
            'y_true': y.values
        }
    
    def analyze_with_shap(self, df, model_results):
        """5) SHAP –∞–Ω–∞–ª–∏–∑ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"""
        
        print("\nüìä –≠–¢–ê–ü 4: SHAP –ê–ù–ê–õ–ò–ó")
        print("="*30)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å –¥–ª—è SHAP
        model = model_results['models'][-1]
        feature_names = model_results['feature_names']
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è SHAP
        exclude_cols = [
            'restaurant_id', 'stat_date', 'total_sales', 'total_orders',
            'latitude', 'longitude', 'location_region', 'rain_category',
            'gojek_accepting_time', 'gojek_preparation_time', 'gojek_delivery_time'
        ]
        
        df_clean = df.dropna(subset=feature_names + ['total_sales'])
        X_shap = df_clean[feature_names]
        
        print(f"üîç –°–æ–∑–¥–∞–Ω–∏–µ SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–π...")
        print(f"   –î–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(X_shap):,}")
        
        # –°–æ–∑–¥–∞–µ–º SHAP explainer
        explainer = shap.TreeExplainer(model)
        
        # –í—ã—á–∏—Å–ª—è–µ–º SHAP values (–¥–ª—è –ø–µ—Ä–≤—ã—Ö 1000 –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        sample_size = min(1000, len(X_shap))
        shap_values = explainer.shap_values(X_shap.iloc[:sample_size])
        
        self.shap_explainer = explainer
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importance()
        }).sort_values('importance', ascending=False)
        
        self.feature_importance = feature_importance
        
        print(f"\nüèÜ –¢–û–ü-15 –í–ê–ñ–ù–´–• –§–ê–ö–¢–û–†–û–í:")
        for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):
            print(f"   {i+1:2}. {row['feature']:25}: {row['importance']:>8.0f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SHAP —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        shap_results = {
            'explainer': explainer,
            'shap_values': shap_values,
            'feature_importance': feature_importance,
            'sample_data': X_shap.iloc[:sample_size]
        }
        
        return shap_results
    
    def generate_professional_report(self, df, model_results, shap_results):
        """7) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        
        print("\nüìã –≠–¢–ê–ü 5: –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–ï–¢–ê")
        print("="*35)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ö—É–¥—à–∏–µ –¥–Ω–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        df_with_pred = df.copy()
        df_clean = df.dropna(subset=model_results['feature_names'] + ['total_sales'])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if len(model_results['oof_predictions']) == len(df_clean):
            df_clean['predicted_sales'] = model_results['oof_predictions']
            df_clean['sales_error'] = df_clean['total_sales'] - df_clean['predicted_sales']
            df_clean['sales_error_pct'] = (df_clean['sales_error'] / df_clean['total_sales']) * 100
        
        # –•—É–¥—à–∏–µ –¥–Ω–∏ (–≥–¥–µ —Ñ–∞–∫—Ç —Å–∏–ª—å–Ω–æ –Ω–∏–∂–µ –ø—Ä–æ–≥–Ω–æ–∑–∞)
        worst_days = df_clean.nlargest(10, 'sales_error_pct')[['restaurant_id', 'stat_date', 'total_sales', 'predicted_sales', 'sales_error', 'sales_error_pct']]
        
        print(f"üîç –¢–û–ü-5 –ü–†–û–ë–õ–ï–ú–ù–´–• –î–ù–ï–ô:")
        for i, (_, row) in enumerate(worst_days.head(5).iterrows()):
            print(f"   {i+1}. {row['stat_date'].date()}: —Ñ–∞–∫—Ç {row['total_sales']:,.0f}, –ø—Ä–æ–≥–Ω–æ–∑ {row['predicted_sales']:,.0f} ({row['sales_error_pct']:+.1f}%)")
        
        report = {
            'model_performance': {
                'mae': model_results['overall_mae'],
                'r2': model_results['overall_r2'],
                'cv_scores': model_results['cv_scores']
            },
            'feature_importance': shap_results['feature_importance'].to_dict('records'),
            'worst_days': worst_days.to_dict('records'),
            'weather_validation': self._validate_weather_impact(df_clean, model_results),
            'recommendations': self._generate_recommendations(shap_results['feature_importance'])
        }
        
        return report
    
    def _validate_weather_impact(self, df, model_results):
        """6) –ü—Ä–æ–≤–µ—Ä–∫–∞ '—á–µ—Å—Ç–Ω–æ—Å—Ç–∏' –ø–æ–≥–æ–¥—ã"""
        
        print(f"\nüå§Ô∏è –í–ê–õ–ò–î–ê–¶–ò–Ø –í–õ–ò–Ø–ù–ò–Ø –ü–û–ì–û–î–´:")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–∂–¥—é
        df['rain_group'] = pd.cut(df['weather_rain'], bins=[0, 0.1, 10, 1000], labels=['no_rain', 'light_rain', 'heavy_rain'])
        
        rain_impact = df.groupby('rain_group').agg({
            'total_sales': ['mean', 'count'],
            'weather_temp': 'mean'
        }).round(0)
        
        print(f"   –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–æ–∂–¥—é:")
        for rain_type in ['no_rain', 'light_rain', 'heavy_rain']:
            if rain_type in rain_impact.index:
                sales = rain_impact.loc[rain_type, ('total_sales', 'mean')]
                count = rain_impact.loc[rain_type, ('total_sales', 'count')]
                print(f"   {rain_type:12}: {sales:,.0f} IDR (–¥–Ω–µ–π: {count})")
        
        return rain_impact.to_dict()
    
    def _generate_recommendations(self, feature_importance):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π"""
        
        top_features = feature_importance.head(10)
        
        recommendations = []
        
        for _, row in top_features.iterrows():
            feature = row['feature']
            
            if 'weather' in feature:
                recommendations.append(f"–ü–æ–≥–æ–¥–∞ ('{feature}') –∑–Ω–∞—á–∏–º–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏ - —É—á–∏—Ç—ã–≤–∞—Ç—å –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏")
            elif 'ads' in feature:
                recommendations.append(f"–†–µ–∫–ª–∞–º–∞ ('{feature}') –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±—é–¥–∂–µ—Ç")
            elif 'rating' in feature:
                recommendations.append(f"–†–µ–π—Ç–∏–Ω–≥ ('{feature}') –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏ - —É–ª—É—á—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ")
            elif 'offline' in feature:
                recommendations.append(f"–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–ª–∞—Ç—Ñ–æ—Ä–º ('{feature}') –∫—Ä–∏—Ç–∏—á–Ω–∞ - –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–±–æ–∏")
        
        return recommendations[:5]  # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    def save_results(self, model_results, shap_results, report):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        joblib.dump(model_results['models'], 'professional_ml_models.pkl')
        print(f"   ‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: professional_ml_models.pkl")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SHAP
        joblib.dump(shap_results, 'professional_shap_results.pkl')
        print(f"   ‚úÖ SHAP —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: professional_shap_results.pkl")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        import json
        with open('professional_ml_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        print(f"   ‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: professional_ml_report.json")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π ML —Å–∏—Å—Ç–µ–º—ã"""
    
    print("üöÄ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø ML –°–ò–°–¢–ï–ú–ê")
    print("–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É: grab_stats + gojek_stats + restaurants")
    print("="*70)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    ml_system = ProfessionalMLSystem()
    
    # 1) –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if not ml_system.load_and_validate_data():
        print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö!")
        return
    
    # 2) Feature Engineering
    df = ml_system.build_feature_dataset()
    
    # 3) –û–±—É—á–µ–Ω–∏–µ —Å TimeSeriesSplit
    model_results = ml_system.train_with_time_series_cv(df)
    
    # 4) SHAP –∞–Ω–∞–ª–∏–∑
    shap_results = ml_system.analyze_with_shap(df, model_results)
    
    # 5) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = ml_system.generate_professional_report(df, model_results, shap_results)
    
    # 6) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    ml_system.save_results(model_results, shap_results, report)
    
    print(f"\nüéâ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø ML –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
    print(f"   üìä R¬≤: {model_results['overall_r2']:.4f}")
    print(f"   üí∞ MAE: {model_results['overall_mae']:,.0f} IDR")
    print(f"   üéØ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")

if __name__ == "__main__":
    main()