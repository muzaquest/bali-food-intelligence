#!/usr/bin/env python3
"""
ü§ñ 100% ML-–û–ë–û–°–ù–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ê–ù–ê–õ–ò–ó–ê
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–≠–¢–ê–ü 1: –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –†–ï–ê–õ–¨–ù–´–• –ø–æ—Ä–æ–≥–æ–≤ –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
–≠–¢–ê–ü 2: ML –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
–≠–¢–ê–ü 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥

–¶–ï–õ–¨: 100% –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –≤—ã–≤–æ–¥–∞
"""

import sqlite3
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import requests
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class MLDataAnalyzer:
    """100% ML-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤"""
    
    def __init__(self, db_path='database.sqlite'):
        self.db_path = db_path
        self.ml_thresholds = {}
        self.factor_correlations = {}
        self.weather_impact_model = None
        self.trained_model = None
        self.feature_importance = {}
        self.scaler = StandardScaler()
        
    def analyze_all_historical_data(self):
        """–≠–¢–ê–ü 1: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print("ü§ñ –≠–¢–ê–ü 1: –ê–ù–ê–õ–ò–ó –í–°–ï–• –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–•")
        print("=" * 60)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥
        all_data = self._load_all_historical_data()
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –∑–∞–ø–∏—Å–µ–π –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥")
        
        if len(all_data) < 100:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞")
            return False
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –†–ï–ê–õ–¨–ù–´–ï –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
        self._calculate_data_driven_thresholds(all_data)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–Ω–∏ –Ω–µ–¥–µ–ª–∏
        self._analyze_weekday_patterns(all_data)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self._analyze_seasonal_patterns(all_data)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        self._analyze_operational_factors(all_data)
        
        return True
        
    def correlate_all_factors(self, all_data):
        """–≠–¢–ê–ü 2: ML –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        
        print("\nü§ñ –≠–¢–ê–ü 2: ML –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó")
        print("=" * 60)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        correlation_data = self._prepare_correlation_data(all_data)
        
        if len(correlation_data) < 50:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return False
            
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
        correlation_matrix = correlation_data.corr()
        sales_correlations = correlation_matrix['total_sales'].sort_values(key=abs, ascending=False)
        
        print("üìä –ö–û–†–†–ï–õ–Ø–¶–ò–ò –° –ü–†–û–î–ê–ñ–ê–ú–ò (–ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏):")
        for factor, corr in sales_correlations.items():
            if factor != 'total_sales' and abs(corr) > 0.1:
                strength = self._get_correlation_strength(abs(corr))
                direction = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è" if corr > 0 else "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è"
                print(f"   ‚Ä¢ {factor}: {corr:.3f} ({direction}, {strength})")
                self.factor_correlations[factor] = corr
                
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–≥–æ–¥–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        self._analyze_weather_correlations(all_data)
        
        return True
        
    def train_ml_model(self, all_data):
        """–≠–¢–ê–ü 3: –û–±—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π ML –º–æ–¥–µ–ª–∏"""
        
        print("\nü§ñ –≠–¢–ê–ü 3: –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò")
        print("=" * 60)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        X, y, feature_names = self._prepare_ml_features(all_data)
        
        if len(X) < 100:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            return False
            
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.trained_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.trained_model.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        y_pred = self.trained_model.predict(X_test_scaled)
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        
        print(f"üìà –ö–ê–ß–ï–°–¢–í–û –ú–û–î–ï–õ–ò:")
        print(f"   ‚Ä¢ R¬≤ Score: {r2:.3f} ({'–æ—Ç–ª–∏—á–Ω–æ' if r2 > 0.8 else '—Ö–æ—Ä–æ—à–æ' if r2 > 0.6 else '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ'})")
        print(f"   ‚Ä¢ MAE: {mae:,.0f} IDR")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_importance = dict(zip(feature_names, self.trained_model.feature_importances_))
        sorted_importance = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print(f"üìä –í–ê–ñ–ù–û–°–¢–¨ –§–ê–ö–¢–û–†–û–í (–ø–æ –¥–∞–Ω–Ω—ã–º ML):")
        for factor, importance in sorted_importance[:10]:
            print(f"   ‚Ä¢ {factor}: {importance:.3f}")
            
        return True
        
    def _load_all_historical_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ"""
        
        conn = sqlite3.connect(self.db_path)
        
        query = """
        SELECT 
            g.stat_date,
            r.name as restaurant_name,
            
            -- –ü–†–û–î–ê–ñ–ò –ò –ó–ê–ö–ê–ó–´
            COALESCE(g.sales, 0) as grab_sales,
            COALESCE(gj.sales, 0) as gojek_sales,
            COALESCE(g.sales, 0) + COALESCE(gj.sales, 0) as total_sales,
            COALESCE(g.orders, 0) + COALESCE(gj.orders, 0) as total_orders,
            
            -- –°–†–ï–î–ù–ò–ô –ß–ï–ö
            CASE WHEN (COALESCE(g.orders, 0) + COALESCE(gj.orders, 0)) > 0 
                 THEN (COALESCE(g.sales, 0) + COALESCE(gj.sales, 0)) / (COALESCE(g.orders, 0) + COALESCE(gj.orders, 0))
                 ELSE 0 END as avg_order_value,
            
            -- –†–ï–ô–¢–ò–ù–ì–ò
            CASE 
                WHEN g.rating > 0 AND gj.rating > 0 THEN (g.rating + gj.rating) / 2
                WHEN g.rating > 0 THEN g.rating
                WHEN gj.rating > 0 THEN gj.rating
                ELSE 4.5 
            END as avg_rating,
            
            -- –ú–ê–†–ö–ï–¢–ò–ù–ì
            COALESCE(g.ads_spend, 0) + COALESCE(gj.ads_spend, 0) as marketing_spend,
            COALESCE(g.ads_sales, 0) + COALESCE(gj.ads_sales, 0) as marketing_sales,
            CASE WHEN (COALESCE(g.ads_spend, 0) + COALESCE(gj.ads_spend, 0)) > 0
                 THEN (COALESCE(g.ads_sales, 0) + COALESCE(gj.ads_sales, 0)) / (COALESCE(g.ads_spend, 0) + COALESCE(gj.ads_spend, 0))
                 ELSE 0 END as roas,
            
            -- –û–ü–ï–†–ê–¶–ò–û–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´
            COALESCE(g.store_is_closed, 0) + COALESCE(gj.store_is_closed, 0) as total_closed,
            COALESCE(g.out_of_stock, 0) + COALESCE(gj.out_of_stock, 0) as total_out_of_stock,
            COALESCE(g.cancelled_orders, 0) + COALESCE(gj.cancelled_orders, 0) as total_cancelled,
            
            -- –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
            CAST(strftime('%w', g.stat_date) AS INTEGER) as day_of_week,
            CAST(strftime('%m', g.stat_date) AS INTEGER) as month,
            CAST(strftime('%j', g.stat_date) AS INTEGER) as day_of_year,
            
            -- –ö–õ–ò–ï–ù–¢–°–ö–ê–Ø –ë–ê–ó–ê
            COALESCE(g.new_customers, 0) + COALESCE(gj.new_client, 0) as new_customers,
            COALESCE(g.repeated_customers, 0) + COALESCE(gj.returned_client, 0) as returning_customers
            
        FROM grab_stats g
        LEFT JOIN gojek_stats gj ON g.restaurant_id = gj.restaurant_id 
                                 AND g.stat_date = gj.stat_date
        LEFT JOIN restaurants r ON g.restaurant_id = r.id
        WHERE g.stat_date >= '2023-01-01'  -- –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2+ –≥–æ–¥–∞
        AND r.name IS NOT NULL
        AND (g.sales > 0 OR gj.sales > 0)  -- –¢–æ–ª—å–∫–æ –¥–Ω–∏ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏
        ORDER BY g.stat_date
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        return df
        
    def _calculate_data_driven_thresholds(self, data):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print("üìä –†–ê–°–ß–ï–¢ –ü–û–†–û–ì–û–í –ù–ê –û–°–ù–û–í–ï –î–ê–ù–ù–´–•:")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∂ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–≤–∞—Ä—Ç–∏–ª–µ–π)
        sales_q25 = data['total_sales'].quantile(0.25)
        sales_q75 = data['total_sales'].quantile(0.75)
        sales_q90 = data['total_sales'].quantile(0.90)
        
        self.ml_thresholds['low_sales'] = sales_q25
        self.ml_thresholds['high_sales'] = sales_q75
        self.ml_thresholds['excellent_sales'] = sales_q90
        
        print(f"   ‚Ä¢ –ù–∏–∑–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏: < {sales_q25:,.0f} IDR (25-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å)")
        print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏: > {sales_q75:,.0f} IDR (75-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å)")
        print(f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏: > {sales_q90:,.0f} IDR (90-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å)")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞
        aov_data = data[data['avg_order_value'] > 0]
        if len(aov_data) > 0:
            aov_q25 = aov_data['avg_order_value'].quantile(0.25)
            aov_q75 = aov_data['avg_order_value'].quantile(0.75)
            
            self.ml_thresholds['low_aov'] = aov_q25
            self.ml_thresholds['high_aov'] = aov_q75
            
            print(f"   ‚Ä¢ –ù–∏–∑–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫: < {aov_q25:,.0f} IDR")
            print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫: > {aov_q75:,.0f} IDR")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥–∞
        rating_q25 = data['avg_rating'].quantile(0.25)
        rating_q75 = data['avg_rating'].quantile(0.75)
        
        self.ml_thresholds['low_rating'] = rating_q25
        self.ml_thresholds['high_rating'] = rating_q75
        
        print(f"   ‚Ä¢ –ù–∏–∑–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥: < {rating_q25:.2f}")
        print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥: > {rating_q75:.2f}")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è ROAS
        roas_data = data[data['roas'] > 0]
        if len(roas_data) > 0:
            roas_q25 = roas_data['roas'].quantile(0.25)
            roas_q75 = roas_data['roas'].quantile(0.75)
            
            self.ml_thresholds['low_roas'] = roas_q25
            self.ml_thresholds['high_roas'] = roas_q75
            
            print(f"   ‚Ä¢ –ù–∏–∑–∫–∏–π ROAS: < {roas_q25:.2f}x")
            print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∏–π ROAS: > {roas_q75:.2f}x")
            
    def _analyze_weekday_patterns(self, data):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏"""
        
        print("\nüìÖ –ê–ù–ê–õ–ò–ó –î–ù–ï–ô –ù–ï–î–ï–õ–ò:")
        
        weekday_stats = data.groupby('day_of_week')['total_sales'].agg(['mean', 'std', 'count'])
        weekday_names = {0: '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ', 1: '–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', 2: '–í—Ç–æ—Ä–Ω–∏–∫', 3: '–°—Ä–µ–¥–∞', 
                        4: '–ß–µ—Ç–≤–µ—Ä–≥', 5: '–ü—è—Ç–Ω–∏—Ü–∞', 6: '–°—É–±–±–æ—Ç–∞'}
        
        overall_mean = data['total_sales'].mean()
        
        for day_num, stats in weekday_stats.iterrows():
            day_name = weekday_names[day_num]
            deviation = ((stats['mean'] - overall_mean) / overall_mean) * 100
            
            if abs(deviation) > 10:  # –ó–Ω–∞—á–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                trend = "—Å–∏–ª—å–Ω–µ–µ" if deviation > 0 else "—Å–ª–∞–±–µ–µ"
                print(f"   ‚Ä¢ {day_name}: {deviation:+.1f}% –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ ({trend})")
                self.ml_thresholds[f'weekday_{day_num}_factor'] = deviation / 100
                
    def _analyze_seasonal_patterns(self, data):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        
        print("\nüåÖ –ê–ù–ê–õ–ò–ó –°–ï–ó–û–ù–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ –¥–∞—Ç–∞—Ö
        data['stat_date'] = pd.to_datetime(data['stat_date'])
        monthly_stats = data.groupby('month')['total_sales'].agg(['mean', 'count'])
        
        overall_mean = data['total_sales'].mean()
        month_names = {1: '–Ø–Ω–≤–∞—Ä—å', 2: '–§–µ–≤—Ä–∞–ª—å', 3: '–ú–∞—Ä—Ç', 4: '–ê–ø—Ä–µ–ª—å', 5: '–ú–∞–π', 6: '–ò—é–Ω—å',
                      7: '–ò—é–ª—å', 8: '–ê–≤–≥—É—Å—Ç', 9: '–°–µ–Ω—Ç—è–±—Ä—å', 10: '–û–∫—Ç—è–±—Ä—å', 11: '–ù–æ—è–±—Ä—å', 12: '–î–µ–∫–∞–±—Ä—å'}
        
        for month_num, stats in monthly_stats.iterrows():
            if stats['count'] > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                month_name = month_names[month_num]
                deviation = ((stats['mean'] - overall_mean) / overall_mean) * 100
                
                if abs(deviation) > 15:  # –ó–Ω–∞—á–∏–º–æ–µ —Å–µ–∑–æ–Ω–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                    trend = "–≤—ã—Å–æ–∫–∏–π" if deviation > 0 else "–Ω–∏–∑–∫–∏–π"
                    print(f"   ‚Ä¢ {month_name}: {deviation:+.1f}% –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ ({trend} —Å–µ–∑–æ–Ω)")
                    self.ml_thresholds[f'month_{month_num}_factor'] = deviation / 100
                    
    def _analyze_operational_factors(self, data):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        
        print("\nüè™ –ê–ù–ê–õ–ò–ó –û–ü–ï–†–ê–¶–ò–û–ù–ù–´–• –§–ê–ö–¢–û–†–û–í:")
        
        # –í–ª–∏—è–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–∏–π
        closed_impact = self._calculate_factor_impact(data, 'total_closed', 'total_sales')
        if closed_impact['significant']:
            print(f"   ‚Ä¢ –ó–∞–∫—Ä—ã—Ç–∏–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞: {closed_impact['impact']:+.1f}% –∫ –ø—Ä–æ–¥–∞–∂–∞–º")
            self.ml_thresholds['closure_impact'] = closed_impact['impact'] / 100
            
        # –í–ª–∏—è–Ω–∏–µ –¥–µ—Ñ–∏—Ü–∏—Ç–∞ —Ç–æ–≤–∞—Ä–∞
        stock_impact = self._calculate_factor_impact(data, 'total_out_of_stock', 'total_sales')
        if stock_impact['significant']:
            print(f"   ‚Ä¢ –î–µ—Ñ–∏—Ü–∏—Ç —Ç–æ–≤–∞—Ä–∞: {stock_impact['impact']:+.1f}% –∫ –ø—Ä–æ–¥–∞–∂–∞–º")
            self.ml_thresholds['stock_impact'] = stock_impact['impact'] / 100
            
        # –í–ª–∏—è–Ω–∏–µ –æ—Ç–º–µ–Ω
        cancel_impact = self._calculate_factor_impact(data, 'total_cancelled', 'total_sales')
        if cancel_impact['significant']:
            print(f"   ‚Ä¢ –û—Ç–º–µ–Ω—ã –∑–∞–∫–∞–∑–æ–≤: {cancel_impact['impact']:+.1f}% –∫ –ø—Ä–æ–¥–∞–∂–∞–º")
            self.ml_thresholds['cancellation_impact'] = cancel_impact['impact'] / 100
            
    def _calculate_factor_impact(self, data, factor_col, target_col):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é"""
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–Ω–∏ —Å —Ñ–∞–∫—Ç–æ—Ä–æ–º –∏ –±–µ–∑
        with_factor = data[data[factor_col] > 0][target_col].mean()
        without_factor = data[data[factor_col] == 0][target_col].mean()
        
        if without_factor > 0:
            impact = ((with_factor - without_factor) / without_factor) * 100
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            count_with = len(data[data[factor_col] > 0])
            count_without = len(data[data[factor_col] == 0])
            
            significant = count_with > 10 and count_without > 10 and abs(impact) > 5
            
            return {
                'impact': impact,
                'significant': significant,
                'count_with': count_with,
                'count_without': count_without
            }
        
        return {'impact': 0, 'significant': False}
        
    def _prepare_correlation_data(self, data):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        correlation_cols = [
            'total_sales', 'total_orders', 'avg_order_value', 'avg_rating',
            'marketing_spend', 'marketing_sales', 'roas',
            'total_closed', 'total_out_of_stock', 'total_cancelled',
            'day_of_week', 'month', 'new_customers', 'returning_customers'
        ]
        
        return data[correlation_cols].dropna()
        
    def _analyze_weather_correlations(self, data):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –ø–æ–≥–æ–¥–æ–π (–æ–±—Ä–∞–∑–µ—Ü)"""
        
        print("\nüå§Ô∏è –ê–ù–ê–õ–ò–ó –ü–û–ì–û–î–ù–´–• –ö–û–†–†–ï–õ–Ø–¶–ò–ô:")
        print("   ‚Ä¢ –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥")
        print("   ‚Ä¢ –≠—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è, –Ω–æ –¥–∞—Å—Ç —Ç–æ—á–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–æ–∂–¥—å-–ø—Ä–æ–¥–∞–∂–∏")
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ö–æ–¥–∞
        
    def _prepare_ml_features(self, data):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML –º–æ–¥–µ–ª–∏"""
        
        feature_cols = [
            'total_orders', 'avg_order_value', 'avg_rating',
            'marketing_spend', 'roas', 'total_closed', 'total_out_of_stock', 
            'total_cancelled', 'day_of_week', 'month', 'day_of_year',
            'new_customers', 'returning_customers'
        ]
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        clean_data = data[feature_cols + ['total_sales']].dropna()
        
        X = clean_data[feature_cols].values
        y = clean_data['total_sales'].values
        
        return X, y, feature_cols
        
    def _get_correlation_strength(self, corr_value):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∏–ª—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        if corr_value >= 0.7:
            return "–æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è"
        elif corr_value >= 0.5:
            return "—Å–∏–ª—å–Ω–∞—è"
        elif corr_value >= 0.3:
            return "—É–º–µ—Ä–µ–Ω–Ω–∞—è"
        elif corr_value >= 0.1:
            return "—Å–ª–∞–±–∞—è"
        else:
            return "–æ—á–µ–Ω—å —Å–ª–∞–±–∞—è"
            
    def save_ml_insights(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ML –∞–Ω–∞–ª–∏–∑–∞"""
        
        insights = {
            'ml_thresholds': self.ml_thresholds,
            'factor_correlations': self.factor_correlations,
            'feature_importance': self.feature_importance,
            'model_trained': self.trained_model is not None
        }
        
        with open('ml_insights.json', 'w', encoding='utf-8') as f:
            json.dump(insights, f, ensure_ascii=False, indent=2)
            
        print(f"\nüíæ ML –∏–Ω—Å–∞–π—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ml_insights.json")
        
def main():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π ML –∞–Ω–∞–ª–∏–∑"""
    
    print("ü§ñ –ó–ê–ü–£–°–ö 100% ML-–û–ë–û–°–ù–û–í–ê–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 70)
    
    analyzer = MLDataAnalyzer()
    
    # –≠–¢–ê–ü 1: –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    if not analyzer.analyze_all_historical_data():
        return
        
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–æ–≤
    all_data = analyzer._load_all_historical_data()
    
    # –≠–¢–ê–ü 2: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    if not analyzer.correlate_all_factors(all_data):
        return
        
    # –≠–¢–ê–ü 3: –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏
    if not analyzer.train_ml_model(all_data):
        return
        
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    analyzer.save_ml_insights()
    
    print("\nüéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù! –°–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å 100% –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∞ –¥–∞–Ω–Ω—ã–º–∏!")

if __name__ == "__main__":
    main()