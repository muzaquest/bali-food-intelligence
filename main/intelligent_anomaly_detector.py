import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import sqlite3
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest

class IntelligentAnomalyDetector:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –í–°–Å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    """
    
    def __init__(self, db_path: str = "data/database.sqlite"):
        self.db_path = db_path
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.numerical_metrics = [
            'total_sales', 'orders', 'rating', 'delivery_time', 
            'marketing_spend', 'marketing_sales', 'roas', 
            'avg_order_value', 'cancel_rate'
        ]
        
        # –ü–æ—Ä–æ–≥–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        self.significance_thresholds = {
            'critical': 3.0,    # 3 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            'major': 2.0,       # 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è 
            'moderate': 1.5,    # 1.5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            'percentage_change': 20.0,  # 20% –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            'correlation_strength': 0.7  # –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
        }
    
    def analyze_everything(self, start_date: str, end_date: str, 
                          comparison_start: str = None, comparison_end: str = None) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –í–°–Å –∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        """
        
        print(f"üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –ê–ù–û–ú–ê–õ–ò–ô –ò –¢–†–ï–ù–î–û–í")
        print(f"üìÖ –û—Å–Ω–æ–≤–Ω–æ–π –ø–µ—Ä–∏–æ–¥: {start_date} ‚Üí {end_date}")
        if comparison_start and comparison_end:
            print(f"üìÖ –ü–µ—Ä–∏–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {comparison_start} ‚Üí {comparison_end}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        current_data = self._load_comprehensive_data(start_date, end_date)
        comparison_data = None
        if comparison_start and comparison_end:
            comparison_data = self._load_comprehensive_data(comparison_start, comparison_end)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞
        findings = {
            'timestamp': datetime.now().isoformat(),
            'period': f"{start_date} to {end_date}",
            'critical_findings': [],
            'major_findings': [],
            'interesting_patterns': [],
            'hidden_correlations': [],
            'performance_anomalies': [],
            'trend_changes': [],
            'outlier_restaurants': [],
            'platform_insights': [],
            'seasonal_discoveries': [],
            'unexpected_behaviors': []
        }
        
        try:
            # 1. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã)
            findings['performance_anomalies'] = self._detect_statistical_anomalies(current_data)
            
            # 2. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–Ω–µ–æ–±—ã—á–Ω—ã–µ –≥—Ä—É–ø–ø—ã)
            findings['outlier_restaurants'] = self._detect_clustering_anomalies(current_data)
            
            # 3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–Ω–µ–æ–±—ã—á–Ω—ã–µ –¥–Ω–∏/–Ω–µ–¥–µ–ª–∏)
            findings['seasonal_discoveries'] = self._detect_temporal_anomalies(current_data)
            
            # 4. –°–∫—Ä—ã—Ç—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏
            findings['hidden_correlations'] = self._discover_hidden_correlations(current_data)
            
            # 5. –ê–Ω–æ–º–∞–ª–∏–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º
            findings['platform_insights'] = self._analyze_platform_anomalies(current_data)
            
            # 6. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
            if comparison_data is not None:
                findings['trend_changes'] = self._detect_trend_changes(current_data, comparison_data)
            
            # 7. –ù–µ–æ–±—ã—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤
            findings['unexpected_behaviors'] = self._detect_behavioral_anomalies(current_data)
            
            # 8. –†–∞–Ω–∂–∏—Ä—É–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            findings = self._rank_findings_by_importance(findings)
            
            return findings
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return findings
    
    def _load_comprehensive_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥"""
        
        conn = sqlite3.connect(self.db_path)
        
        query = """
        SELECT 
            restaurant_name,
            date,
            platform,
            total_sales,
            orders,
            rating,
            delivery_time,
            marketing_spend,
            marketing_sales,
            marketing_orders,
            roas,
            avg_order_value,
            cancel_rate,
            ads_on,
            weather_condition,
            temperature_celsius,
            precipitation_mm,
            is_holiday,
            is_weekend
        FROM restaurant_data 
        WHERE date BETWEEN ? AND ?
        ORDER BY date, restaurant_name, platform
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date, end_date))
        conn.close()
        
        df['date'] = pd.to_datetime(df['date'])
        df['weekday'] = df['date'].dt.day_name()
        df['month'] = df['date'].dt.month
        df['week'] = df['date'].dt.isocalendar().week
        
        return df
    
    def _detect_statistical_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –ª—é–±—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö"""
        
        anomalies = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∏—Å–ª–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É
        for metric in self.numerical_metrics:
            if metric in data.columns and data[metric].notna().sum() > 10:
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Isolation Forest –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π
                values = data[data[metric].notna()][metric].values.reshape(-1, 1)
                
                if len(values) > 20:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    iso_forest = IsolationForest(contamination=0.1, random_state=42)
                    outliers = iso_forest.fit_predict(values)
                    
                    # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
                    anomaly_indices = np.where(outliers == -1)[0]
                    
                    if len(anomaly_indices) > 0:
                        metric_data = data[data[metric].notna()].iloc[anomaly_indices]
                        
                        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞–º
                        restaurant_anomalies = {}
                        for _, row in metric_data.iterrows():
                            restaurant = row['restaurant_name']
                            if restaurant not in restaurant_anomalies:
                                restaurant_anomalies[restaurant] = []
                            restaurant_anomalies[restaurant].append({
                                'metric': metric,
                                'value': row[metric],
                                'date': row['date'].strftime('%Y-%m-%d'),
                                'platform': row['platform']
                            })
                        
                        # –°–æ–∑–¥–∞–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏
                        for restaurant, rest_anomalies in restaurant_anomalies.items():
                            if len(rest_anomalies) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 –∞–Ω–æ–º–∞–ª–∏–∏
                                severity = self._calculate_severity(len(rest_anomalies), len(data[data['restaurant_name'] == restaurant]))
                                
                                anomalies.append({
                                    'type': 'statistical_anomaly',
                                    'severity': severity,
                                    'restaurant': restaurant,
                                    'metric': metric,
                                    'anomaly_count': len(rest_anomalies),
                                    'details': rest_anomalies[:5],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
                                    'description': f"–†–µ—Å—Ç–æ—Ä–∞–Ω {restaurant} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç {len(rest_anomalies)} –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –º–µ—Ç—Ä–∏–∫–µ {metric}"
                                })
        
        return sorted(anomalies, key=lambda x: x['severity'], reverse=True)
    
    def _detect_clustering_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã —Å –Ω–µ–æ–±—ã—á–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"""
        
        outliers = []
        
        try:
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞–º
            restaurant_features = data.groupby('restaurant_name').agg({
                'total_sales': ['mean', 'std', 'sum'],
                'orders': ['mean', 'std', 'sum'],
                'rating': ['mean', 'std'],
                'delivery_time': ['mean', 'std'],
                'roas': ['mean', 'std'],
                'avg_order_value': ['mean', 'std'],
                'cancel_rate': ['mean', 'std']
            }).fillna(0)
            
            # –£–ø—Ä–æ—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            restaurant_features.columns = ['_'.join(col) for col in restaurant_features.columns]
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(restaurant_features)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º DBSCAN –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
            dbscan = DBSCAN(eps=1.5, min_samples=3)
            clusters = dbscan.fit_predict(features_scaled)
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–æ—Å—ã (–∫–ª–∞—Å—Ç–µ—Ä -1)
            outlier_mask = clusters == -1
            outlier_restaurants = restaurant_features.index[outlier_mask].tolist()
            
            for restaurant in outlier_restaurants:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—á–µ–º—É —ç—Ç–æ—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω —è–≤–ª—è–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å–æ–º
                rest_data = data[data['restaurant_name'] == restaurant]
                
                unusual_metrics = []
                for metric in self.numerical_metrics:
                    if metric in rest_data.columns:
                        rest_values = rest_data[metric].dropna()
                        if len(rest_values) > 0:
                            market_values = data[data['restaurant_name'] != restaurant][metric].dropna()
                            
                            if len(market_values) > 0:
                                rest_mean = rest_values.mean()
                                market_mean = market_values.mean()
                                market_std = market_values.std()
                                
                                if market_std > 0:
                                    z_score = abs(rest_mean - market_mean) / market_std
                                    if z_score > 2:  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                                        unusual_metrics.append({
                                            'metric': metric,
                                            'restaurant_value': rest_mean,
                                            'market_average': market_mean,
                                            'deviation': z_score
                                        })
                
                if unusual_metrics:
                    severity = self._calculate_severity(len(unusual_metrics), len(self.numerical_metrics))
                    
                    outliers.append({
                        'type': 'behavioral_outlier',
                        'severity': severity,
                        'restaurant': restaurant,
                        'unusual_metrics': unusual_metrics,
                        'description': f"–†–µ—Å—Ç–æ—Ä–∞–Ω {restaurant} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–æ–±—ã—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ {len(unusual_metrics)} –º–µ—Ç—Ä–∏–∫–∞–º"
                    })
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return sorted(outliers, key=lambda x: x['severity'], reverse=True)
    
    def _detect_temporal_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–µ–æ–±—ã—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        
        temporal_anomalies = []
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        weekday_analysis = self._analyze_weekday_patterns(data)
        temporal_anomalies.extend(weekday_analysis)
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –Ω–µ–¥–µ–ª—è–º
        weekly_analysis = self._analyze_weekly_patterns(data)
        temporal_anomalies.extend(weekly_analysis)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã—Ö –¥–Ω–µ–π
        holiday_analysis = self._analyze_holiday_patterns(data)
        temporal_anomalies.extend(holiday_analysis)
        
        return sorted(temporal_anomalies, key=lambda x: x['severity'], reverse=True)
    
    def _analyze_weekday_patterns(self, data: pd.DataFrame) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏"""
        
        patterns = []
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        weekday_stats = data.groupby('weekday').agg({
            'total_sales': ['mean', 'std'],
            'orders': ['mean', 'std'],
            'rating': ['mean', 'std']
        })
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–Ω–∏ —Å –Ω–µ–æ–±—ã—á–Ω—ã–º–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
        for metric in ['total_sales', 'orders', 'rating']:
            if (metric, 'mean') in weekday_stats.columns:
                means = weekday_stats[(metric, 'mean')]
                stds = weekday_stats[(metric, 'std')]
                
                overall_mean = means.mean()
                overall_std = means.std()
                
                for weekday in means.index:
                    if overall_std > 0:
                        z_score = abs(means[weekday] - overall_mean) / overall_std
                        
                        if z_score > 1.5:  # –ù–µ–æ–±—ã—á–Ω—ã–π –¥–µ–Ω—å
                            direction = "–≤—ã—à–µ" if means[weekday] > overall_mean else "–Ω–∏–∂–µ"
                            percentage = abs((means[weekday] / overall_mean - 1) * 100)
                            
                            patterns.append({
                                'type': 'weekday_anomaly',
                                'severity': min(z_score / 3.0, 1.0),
                                'weekday': weekday,
                                'metric': metric,
                                'value': means[weekday],
                                'market_average': overall_mean,
                                'deviation_percent': percentage,
                                'description': f"{weekday} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç {metric} –Ω–∞ {percentage:.1f}% {direction} —Å—Ä–µ–¥–Ω–µ–≥–æ"
                            })
        
        return patterns
    
    def _analyze_weekly_patterns(self, data: pd.DataFrame) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ –Ω–µ–¥–µ–ª—è–º"""
        
        patterns = []
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –Ω–µ–¥–µ–ª—è–º
        weekly_data = data.groupby(['week']).agg({
            'total_sales': 'sum',
            'orders': 'sum'
        }).reset_index()
        
        for metric in ['total_sales', 'orders']:
            if metric in weekly_data.columns:
                values = weekly_data[metric]
                mean_val = values.mean()
                std_val = values.std()
                
                if std_val > 0:
                    # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–¥–µ–ª–∏ —Å –∞–Ω–æ–º–∞–ª—å–Ω—ã–º–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
                    for _, week_data in weekly_data.iterrows():
                        z_score = abs(week_data[metric] - mean_val) / std_val
                        
                        if z_score > 2:  # –ê–Ω–æ–º–∞–ª—å–Ω–∞—è –Ω–µ–¥–µ–ª—è
                            direction = "–≤—ã—à–µ" if week_data[metric] > mean_val else "–Ω–∏–∂–µ"
                            percentage = abs((week_data[metric] / mean_val - 1) * 100)
                            
                            patterns.append({
                                'type': 'weekly_anomaly',
                                'severity': min(z_score / 3.0, 1.0),
                                'week': week_data['week'],
                                'metric': metric,
                                'value': week_data[metric],
                                'market_average': mean_val,
                                'deviation_percent': percentage,
                                'description': f"–ù–µ–¥–µ–ª—è {week_data['week']} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç {metric} –Ω–∞ {percentage:.1f}% {direction} —Å—Ä–µ–¥–Ω–µ–≥–æ"
                            })
        
        return patterns
    
    def _analyze_holiday_patterns(self, data: pd.DataFrame) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤"""
        
        patterns = []
        
        if 'is_holiday' in data.columns:
            holiday_data = data[data['is_holiday'] == 1]
            normal_data = data[data['is_holiday'] == 0]
            
            if len(holiday_data) > 0 and len(normal_data) > 0:
                for metric in ['total_sales', 'orders']:
                    if metric in data.columns:
                        holiday_mean = holiday_data.groupby('date')[metric].sum().mean()
                        normal_mean = normal_data.groupby('date')[metric].sum().mean()
                        
                        if normal_mean > 0:
                            impact = ((holiday_mean / normal_mean) - 1) * 100
                            
                            if abs(impact) > 20:  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤
                                patterns.append({
                                    'type': 'holiday_impact',
                                    'severity': min(abs(impact) / 100, 1.0),
                                    'metric': metric,
                                    'holiday_average': holiday_mean,
                                    'normal_average': normal_mean,
                                    'impact_percent': impact,
                                    'description': f"–ü—Ä–∞–∑–¥–Ω–∏–∫–∏ –∏–∑–º–µ–Ω—è—é—Ç {metric} –Ω–∞ {impact:+.1f}%"
                                })
        
        return patterns
    
    def _discover_hidden_correlations(self, data: pd.DataFrame) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–∫—Ä—ã—Ç—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        
        correlations = []
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        numeric_data = data[self.numerical_metrics].dropna()
        
        if len(numeric_data) > 20:
            corr_matrix = numeric_data.corr()
            
            # –ò—â–µ–º —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–∏—Å–∫–ª—é—á–∞—è –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏)
            for i, metric1 in enumerate(corr_matrix.columns):
                for j, metric2 in enumerate(corr_matrix.columns):
                    if i < j:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
                        correlation = corr_matrix.loc[metric1, metric2]
                        
                        if abs(correlation) > self.significance_thresholds['correlation_strength']:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π
                            if self._is_unexpected_correlation(metric1, metric2, correlation):
                                correlations.append({
                                    'type': 'hidden_correlation',
                                    'severity': abs(correlation),
                                    'metric1': metric1,
                                    'metric2': metric2,
                                    'correlation': correlation,
                                    'strength': '—Å–∏–ª—å–Ω–∞—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è' if correlation > 0 else '—Å–∏–ª—å–Ω–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è',
                                    'description': f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è {('–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è' if correlation > 0 else '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è')} –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É {metric1} –∏ {metric2} ({correlation:.3f})"
                                })
        
        return sorted(correlations, key=lambda x: x['severity'], reverse=True)
    
    def _analyze_platform_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –º–µ–∂–¥—É –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏"""
        
        platform_insights = []
        
        if 'platform' in data.columns:
            platforms = data['platform'].unique()
            
            for metric in self.numerical_metrics:
                if metric in data.columns:
                    platform_stats = data.groupby('platform')[metric].agg(['mean', 'std']).fillna(0)
                    
                    if len(platforms) >= 2:
                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Å–µ –ø–∞—Ä—ã –ø–ª–∞—Ç—Ñ–æ—Ä–º
                        for i, platform1 in enumerate(platforms):
                            for platform2 in platforms[i+1:]:
                                if platform1 in platform_stats.index and platform2 in platform_stats.index:
                                    mean1 = platform_stats.loc[platform1, 'mean']
                                    mean2 = platform_stats.loc[platform2, 'mean']
                                    
                                    if mean1 > 0 and mean2 > 0:
                                        ratio = max(mean1, mean2) / min(mean1, mean2)
                                        
                                        if ratio > 2:  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏
                                            better_platform = platform1 if mean1 > mean2 else platform2
                                            worse_platform = platform2 if mean1 > mean2 else platform1
                                            percentage_diff = ((max(mean1, mean2) / min(mean1, mean2)) - 1) * 100
                                            
                                            platform_insights.append({
                                                'type': 'platform_disparity',
                                                'severity': min(ratio / 5.0, 1.0),
                                                'metric': metric,
                                                'better_platform': better_platform,
                                                'worse_platform': worse_platform,
                                                'ratio': ratio,
                                                'percentage_diff': percentage_diff,
                                                'description': f"–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ {better_platform} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç {metric} –≤ {ratio:.1f} —Ä–∞–∑–∞ –ª—É—á—à–µ —á–µ–º {worse_platform}"
                                            })
        
        return sorted(platform_insights, key=lambda x: x['severity'], reverse=True)
    
    def _detect_trend_changes(self, current_data: pd.DataFrame, comparison_data: pd.DataFrame) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ –º–µ–∂–¥—É –ø–µ—Ä–∏–æ–¥–∞–º–∏"""
        
        trend_changes = []
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        current_agg = current_data.groupby('restaurant_name')[self.numerical_metrics].mean()
        comparison_agg = comparison_data.groupby('restaurant_name')[self.numerical_metrics].mean()
        
        common_restaurants = set(current_agg.index) & set(comparison_agg.index)
        
        for restaurant in common_restaurants:
            for metric in self.numerical_metrics:
                if metric in current_agg.columns and metric in comparison_agg.columns:
                    current_val = current_agg.loc[restaurant, metric]
                    comparison_val = comparison_agg.loc[restaurant, metric]
                    
                    if pd.notna(current_val) and pd.notna(comparison_val) and comparison_val != 0:
                        change_percent = ((current_val / comparison_val) - 1) * 100
                        
                        if abs(change_percent) > self.significance_thresholds['percentage_change']:
                            trend_changes.append({
                                'type': 'trend_change',
                                'severity': min(abs(change_percent) / 100, 1.0),
                                'restaurant': restaurant,
                                'metric': metric,
                                'current_value': current_val,
                                'previous_value': comparison_val,
                                'change_percent': change_percent,
                                'direction': '—É–ª—É—á—à–µ–Ω–∏–µ' if change_percent > 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ',
                                'description': f"{restaurant}: {metric} –∏–∑–º–µ–Ω–∏–ª—Å—è –Ω–∞ {change_percent:+.1f}%"
                            })
        
        return sorted(trend_changes, key=lambda x: x['severity'], reverse=True)
    
    def _detect_behavioral_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –Ω–µ–æ–±—ã—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤"""
        
        behaviors = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –Ω–µ–æ–±—ã—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
        for restaurant in data['restaurant_name'].unique():
            rest_data = data[data['restaurant_name'] == restaurant]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫
            inconsistencies = self._check_metric_consistency(rest_data)
            if inconsistencies:
                behaviors.extend(inconsistencies)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–∑–æ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
            seasonal_anomalies = self._check_seasonal_consistency(rest_data, restaurant)
            if seasonal_anomalies:
                behaviors.extend(seasonal_anomalies)
        
        return sorted(behaviors, key=lambda x: x['severity'], reverse=True)
    
    def _check_metric_consistency(self, restaurant_data: pd.DataFrame) -> List[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –≤–Ω—É—Ç—Ä–∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞"""
        
        inconsistencies = []
        restaurant_name = restaurant_data['restaurant_name'].iloc[0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –∏ –∑–∞–∫–∞–∑–æ–≤ (—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫)
        if 'total_sales' in restaurant_data.columns and 'orders' in restaurant_data.columns:
            daily_data = restaurant_data.groupby('date').agg({
                'total_sales': 'sum',
                'orders': 'sum'
            })
            
            daily_data = daily_data[daily_data['orders'] > 0]
            if len(daily_data) > 10:
                daily_data['avg_order_value'] = daily_data['total_sales'] / daily_data['orders']
                
                # –ò—â–µ–º –¥–Ω–∏ —Å –∞–Ω–æ–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º —á–µ–∫–æ–º
                aov_mean = daily_data['avg_order_value'].mean()
                aov_std = daily_data['avg_order_value'].std()
                
                if aov_std > 0:
                    anomalous_days = daily_data[abs(daily_data['avg_order_value'] - aov_mean) > 2 * aov_std]
                    
                    if len(anomalous_days) > 2:
                        inconsistencies.append({
                            'type': 'metric_inconsistency',
                            'severity': min(len(anomalous_days) / len(daily_data), 1.0),
                            'restaurant': restaurant_name,
                            'issue': 'volatile_order_value',
                            'anomalous_days': len(anomalous_days),
                            'total_days': len(daily_data),
                            'description': f"{restaurant_name}: {len(anomalous_days)} –¥–Ω–µ–π —Å –∞–Ω–æ–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º —á–µ–∫–æ–º"
                        })
        
        return inconsistencies
    
    def _check_seasonal_consistency(self, restaurant_data: pd.DataFrame, restaurant_name: str) -> List[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–∑–æ–Ω–Ω—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞"""
        
        anomalies = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        if 'total_sales' in restaurant_data.columns:
            weekday_sales = restaurant_data.groupby('weekday')['total_sales'].mean()
            
            if len(weekday_sales) >= 6:  # –ï—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏
                sales_std = weekday_sales.std()
                sales_mean = weekday_sales.mean()
                
                if sales_mean > 0:
                    cv = sales_std / sales_mean  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
                    
                    if cv > 0.5:  # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
                        anomalies.append({
                            'type': 'seasonal_inconsistency',
                            'severity': min(cv, 1.0),
                            'restaurant': restaurant_name,
                            'issue': 'high_weekday_variability',
                            'coefficient_variation': cv,
                            'description': f"{restaurant_name}: –æ—á–µ–Ω—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏ (CV={cv:.2f})"
                        })
        
        return anomalies
    
    def _rank_findings_by_importance(self, findings: Dict) -> Dict:
        """–†–∞–Ω–∂–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        
        all_findings = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞—Ö–æ–¥–∫–∏ —Å –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç—å—é
        for category, finding_list in findings.items():
            if isinstance(finding_list, list):
                for finding in finding_list:
                    if isinstance(finding, dict) and 'severity' in finding:
                        finding['category'] = category
                        all_findings.append(finding)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        all_findings.sort(key=lambda x: x['severity'], reverse=True)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        for finding in all_findings:
            if finding['severity'] >= 0.8:
                findings['critical_findings'].append(finding)
            elif finding['severity'] >= 0.5:
                findings['major_findings'].append(finding)
            else:
                findings['interesting_patterns'].append(finding)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Ö–æ–¥–æ–∫ –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for category in ['critical_findings', 'major_findings', 'interesting_patterns']:
            findings[category] = findings[category][:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        
        return findings
    
    def _calculate_severity(self, anomaly_count: int, total_count: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏"""
        if total_count == 0:
            return 0.0
        
        ratio = anomaly_count / total_count
        return min(ratio * 2, 1.0)  # –ú–∞–∫—Å–∏–º—É–º 1.0
    
    def _is_unexpected_correlation(self, metric1: str, metric2: str, correlation: float) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        expected_positive = [
            ('total_sales', 'orders'),
            ('total_sales', 'marketing_spend'),
            ('marketing_spend', 'marketing_sales'),
            ('rating', 'total_sales')
        ]
        
        expected_negative = [
            ('rating', 'delivery_time'),
            ('delivery_time', 'total_sales'),
            ('cancel_rate', 'rating'),
            ('cancel_rate', 'total_sales')
        ]
        
        pair = (metric1, metric2) if metric1 < metric2 else (metric2, metric1)
        
        # –ï—Å–ª–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –æ–∂–∏–¥–∞–µ–º–∞—è, —Ç–æ –æ–Ω–∞ –Ω–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è
        if correlation > 0 and pair in expected_positive:
            return False
        if correlation < 0 and pair in expected_negative:
            return False
        
        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å—á–∏—Ç–∞–µ–º –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–º–∏
        return True
    
    def generate_intelligent_report(self, findings: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ"""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ïë                    üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó - –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –ê–ù–û–ú–ê–õ–ò–ô
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ïë üìÖ –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: {findings['period']}
‚ïë üïê –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {timestamp}
‚ïë üéØ –ù–∞–π–¥–µ–Ω–æ: {len(findings['critical_findings'])} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö, {len(findings['major_findings'])} –≤–∞–∂–Ω—ã—Ö, {len(findings['interesting_patterns'])} –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ù–ê–•–û–î–ö–ò (—Ç—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        if findings['critical_findings']:
            for i, finding in enumerate(findings['critical_findings'], 1):
                report += f"\n{i}. üî¥ {finding['description']}\n"
                report += f"   –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {finding['severity']:.1%} | –¢–∏–ø: {finding['type']}\n"
                if 'restaurant' in finding:
                    report += f"   –†–µ—Å—Ç–æ—Ä–∞–Ω: {finding['restaurant']}\n"
        else:
            report += "\n‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n"
        
        report += f"""

‚ö†Ô∏è –í–ê–ñ–ù–´–ï –ù–ê–•–û–î–ö–ò (—Ç—Ä–µ–±—É—é—Ç –∞–Ω–∞–ª–∏–∑–∞)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        if findings['major_findings']:
            for i, finding in enumerate(findings['major_findings'], 1):
                report += f"\n{i}. üü° {finding['description']}\n"
                report += f"   –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {finding['severity']:.1%} | –¢–∏–ø: {finding['type']}\n"
        else:
            report += "\n‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n"
        
        report += f"""

üí° –ò–ù–¢–ï–†–ï–°–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ (–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        if findings['interesting_patterns']:
            for i, finding in enumerate(findings['interesting_patterns'], 1):
                report += f"\n{i}. üîç {finding['description']}\n"
                report += f"   –¢–∏–ø: {finding['type']}\n"
        else:
            report += "\nüìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –Ω–∞—Ö–æ–¥–æ–∫
        if findings['hidden_correlations']:
            report += f"""

üîó –°–ö–†–´–¢–´–ï –ö–û–†–†–ï–õ–Ø–¶–ò–ò
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
            for correlation in findings['hidden_correlations'][:5]:
                report += f"‚Ä¢ {correlation['description']}\n"
        
        if findings['platform_insights']:
            report += f"""

üì± –ò–ù–°–ê–ô–¢–´ –ü–õ–ê–¢–§–û–†–ú
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
            for insight in findings['platform_insights'][:5]:
                report += f"‚Ä¢ {insight['description']}\n"
        
        report += f"""

üéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Ö–æ–¥–æ–∫
        recommendations = self._generate_automatic_recommendations(findings)
        for i, rec in enumerate(recommendations, 1):
            report += f"{i}. {rec}\n"
        
        report += f"""

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ïë                           üéØ –ö–û–ù–ï–¶ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        return report
    
    def _generate_automatic_recommendations(self, findings: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Ö–æ–¥–æ–∫"""
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–æ–∫
        for finding in findings['critical_findings']:
            if finding['type'] == 'platform_disparity':
                recommendations.append(f"–ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±—é–¥–∂–µ—Ç –≤ –ø–æ–ª—å–∑—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã {finding['better_platform']} (–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ {finding['percentage_diff']:.0f}%)")
            
            elif finding['type'] == 'statistical_anomaly':
                recommendations.append(f"–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞—É–¥–∏—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ {finding['restaurant']} –ø–æ –º–µ—Ç—Ä–∏–∫–µ {finding['metric']}")
            
            elif finding['type'] == 'trend_change' and finding['change_percent'] < -30:
                recommendations.append(f"–°—Ä–æ—á–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å –ø–∞–¥–µ–Ω–∏–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ {finding['restaurant']}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        for correlation in findings['hidden_correlations']:
            if correlation['correlation'] > 0.8:
                recommendations.append(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É {correlation['metric1']} –∏ {correlation['metric2']} –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(findings['critical_findings']) == 0:
            recommendations.append("–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ - –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        if not recommendations:
            recommendations.append("–†–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º")
        
        return recommendations[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π