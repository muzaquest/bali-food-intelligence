#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import time
import os
import sys
import psutil
import pandas as pd
import numpy as np
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def measure_time(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        execution_time = end_time - start_time
        print(f"‚è±Ô∏è {func.__name__}: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
        return result, execution_time
    return wrapper

def get_memory_usage():
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB

def check_file_sizes():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏"""
    print("\nüì¶ –†–ê–ó–ú–ï–†–´ –§–ê–ô–õ–û–í:")
    print("=" * 50)
    
    files_to_check = [
        'client_sales_model.joblib',
        'scaler.joblib',
        'data/database.sqlite'
    ]
    
    total_size = 0
    for file_path in files_to_check:
        if os.path.exists(file_path):
            size_mb = os.path.getsize(file_path) / 1024 / 1024
            total_size += size_mb
            print(f"  üìÑ {file_path}: {size_mb:.2f} MB")
        else:
            print(f"  ‚ùå {file_path}: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    print(f"  üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.2f} MB")
    
    if total_size > 500:
        print("  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö")
    elif total_size > 100:
        print("  üí° –ò–ù–§–û–†–ú–ê–¶–ò–Ø: –†–∞–∑–º–µ—Ä –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã, –Ω–æ —Å—Ç–æ–∏—Ç —Å–ª–µ–¥–∏—Ç—å")
    else:
        print("  ‚úÖ –û–¢–õ–ò–ß–ù–û: –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä")
    
    return total_size

@measure_time
def test_data_loading():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüìä –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•")
    print("=" * 50)
    
    try:
        from main.data_integration import load_data_with_all_features
        
        memory_before = get_memory_usage()
        df = load_data_with_all_features()
        memory_after = get_memory_usage()
        
        memory_used = memory_after - memory_before
        
        print(f"  üìà –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        print(f"  üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π: {len(df.columns)}")
        print(f"  üíæ –ü–∞–º—è—Ç—å –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏: {memory_before:.1f} MB")
        print(f"  üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {memory_after:.1f} MB")
        print(f"  üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_used:.1f} MB")
        
        if memory_used > 500:
            print("  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
        else:
            print("  ‚úÖ –ü–ê–ú–Ø–¢–¨: –í –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã")
        
        return df
        
    except Exception as e:
        print(f"  ‚ùå –û–®–ò–ë–ö–ê: {e}")
        return None

@measure_time 
def test_feature_engineering(df):
    """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è features"""
    print("\nüåü –¢–ï–°–¢ FEATURE ENGINEERING")
    print("=" * 50)
    
    try:
        from main.data_integration import prepare_features_with_all_enhancements
        
        memory_before = get_memory_usage()
        enhanced_df = prepare_features_with_all_enhancements(df.head(1000))  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π
        memory_after = get_memory_usage()
        
        memory_used = memory_after - memory_before
        features_added = len(enhanced_df.columns) - len(df.columns)
        
        print(f"  üîß –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø–æ–ª–µ–π: {len(df.columns)}")
        print(f"  üåü –ò—Ç–æ–≥–æ–≤—ã—Ö –ø–æ–ª–µ–π: {len(enhanced_df.columns)}")
        print(f"  ‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ features: {features_added}")
        print(f"  üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_used:.1f} MB")
        
        return enhanced_df
        
    except Exception as e:
        print(f"  ‚ùå –û–®–ò–ë–ö–ê: {e}")
        return df

@measure_time
def test_model_training():
    """–¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    print("\nü§ñ –¢–ï–°–¢ –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")
    print("=" * 50)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        from main.data_integration import load_data_with_all_features, prepare_features_with_all_enhancements
        
        df = load_data_with_all_features()
        if df.empty:
            print("  ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return None
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
        test_df = df.head(1000)
        enhanced_df = prepare_features_with_all_enhancements(test_df)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        target_column = 'total_sales'
        exclude_columns = ['date', 'restaurant_name', 'platform', 'holiday_name', 
                          'day_name', 'month_name', 'weather_condition', 'temp_category',
                          'rain_category', 'weather_combination', 'special_period_combination',
                          'day_category', target_column]
        
        feature_columns = [col for col in enhanced_df.columns if col not in exclude_columns]
        
        X = enhanced_df[feature_columns].fillna(0)
        y = enhanced_df[target_column]
        
        print(f"  üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ features: {len(feature_columns)}")
        print(f"  üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X)}")
        
        # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ Random Forest
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        memory_before = get_memory_usage()
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        model = RandomForestRegressor(
            n_estimators=10,  # –ú–∞–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
            max_depth=10,     # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
            random_state=42,
            n_jobs=1
        )
        
        model.fit(X_train, y_train)
        
        memory_after = get_memory_usage()
        memory_used = memory_after - memory_before
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        score = model.score(X_test, y_test)
        
        print(f"  üìà R¬≤ Score: {score:.3f}")
        print(f"  üíæ –ü–∞–º—è—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏: {memory_used:.1f} MB")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = pd.DataFrame({
            'feature': feature_columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"  üîù –¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, row in feature_importance.head(5).iterrows():
            print(f"    {row['feature']}: {row['importance']:.4f}")
        
        return model, feature_importance
        
    except Exception as e:
        print(f"  ‚ùå –û–®–ò–ë–ö–ê: {e}")
        print(f"  üìù –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return None, None

@measure_time
def test_report_generation():
    """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞"""
    print("\nüìã –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–ß–Å–¢–ê")
    print("=" * 50)
    
    try:
        from business_intelligence_system import MarketIntelligenceEngine
        
        memory_before = get_memory_usage()
        
        # –°–æ–∑–¥–∞–µ–º engine –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑
        engine = MarketIntelligenceEngine()
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        result = engine.generate_deep_analysis(
            restaurant_name="Ika Canggu",
            start_date="2025-04-01", 
            end_date="2025-04-10"  # –¢–æ–ª—å–∫–æ 10 –¥–Ω–µ–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
        
        memory_after = get_memory_usage()
        memory_used = memory_after - memory_before
        
        print(f"  üíæ –ü–∞–º—è—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {memory_used:.1f} MB")
        
        if 'error' in result:
            print(f"  ‚ùå –û–®–ò–ë–ö–ê: {result['error']}")
            return False
        else:
            print(f"  ‚úÖ –ê–ù–ê–õ–ò–ó: –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
            return True
            
    except Exception as e:
        print(f"  ‚ùå –û–®–ò–ë–ö–ê: {e}")
        return False

def analyze_feature_usage(model, feature_importance_df):
    """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è features"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø FEATURES")
    print("=" * 50)
    
    if model is None or feature_importance_df is None:
        print("  ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    total_features = len(feature_importance_df)
    important_features = len(feature_importance_df[feature_importance_df['importance'] > 0.001])  # > 0.1%
    zero_importance = len(feature_importance_df[feature_importance_df['importance'] == 0])
    
    print(f"  üìä –í—Å–µ–≥–æ features: {total_features}")
    print(f"  ‚úÖ –í–∞–∂–Ω—ã—Ö features (>0.1%): {important_features}")
    print(f"  ‚ùå –ë–µ—Å–ø–æ–ª–µ–∑–Ω—ã—Ö features: {zero_importance}")
    print(f"  üìà –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {(important_features/total_features)*100:.1f}%")
    
    if zero_importance > total_features * 0.3:
        print(f"  ‚ö†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: {zero_importance} features –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è - –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å")
    else:
        print(f"  ‚úÖ –û–¢–õ–ò–ß–ù–û: –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ features –ø–æ–ª–µ–∑–Ω—ã")
    
    # –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤—ã—Ö —Ç–∏–ø–æ–≤ features
    enhanced_feature_types = {
        'weather': [f for f in feature_importance_df['feature'] if any(w in f.lower() for w in ['temp', 'weather', 'rain', 'wind', 'humid'])],
        'calendar': [f for f in feature_importance_df['feature'] if any(c in f.lower() for c in ['holiday', 'weekend', 'month', 'day_of_week'])],
        'customer': [f for f in feature_importance_df['feature'] if any(c in f.lower() for c in ['customer', 'retention', 'new_customer'])],
        'operational': [f for f in feature_importance_df['feature'] if any(o in f.lower() for o in ['efficiency', 'acceptance', 'completion'])],
        'quality': [f for f in feature_importance_df['feature'] if any(q in f.lower() for q in ['rating', 'star', 'quality'])],
        'lag': [f for f in feature_importance_df['feature'] if 'lag' in f.lower()],
        'rolling': [f for f in feature_importance_df['feature'] if 'rolling' in f.lower()]
    }
    
    print(f"\n  üîç –ê–ù–ê–õ–ò–ó –ü–û –¢–ò–ü–ê–ú FEATURES:")
    for feature_type, features in enhanced_feature_types.items():
        if features:
            avg_importance = feature_importance_df[feature_importance_df['feature'].isin(features)]['importance'].mean()
            print(f"    {feature_type}: {len(features)} features, —Å—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {avg_importance:.4f}")

def performance_recommendations():
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
    print("=" * 50)
    
    recommendations = [
        "üîß –î–ª—è Replit: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å max_depth=15, n_estimators=50",
        "üì¶ –°–∂–∞—Ç–∏–µ: –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –º–æ–¥–µ–ª—å —Å compress=3 –≤ joblib",
        "üíæ –ü–∞–º—è—Ç—å: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batch processing –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤",
        "‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: –ö—ç—à–∏—Ä—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è",
        "üóÇÔ∏è Features: –£–¥–∞–ª–∏—Ç–µ features —Å importance < 0.001",
        "üìä –î–∞–Ω–Ω—ã–µ: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ sample –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è/—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
    ]
    
    for rec in recommendations:
        print(f"  {rec}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –°–ò–°–¢–ï–ú–´")
    print("=" * 60)
    print(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤
    total_size = check_file_sizes()
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    df, load_time = test_data_loading()
    
    if df is not None:
        # –¢–µ—Å—Ç feature engineering  
        enhanced_df, fe_time = test_feature_engineering(df)
        
        # –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        result = test_model_training()
        if result[0] is not None:
            (model, feature_importance), train_time = result
        else:
            model, feature_importance, train_time = None, None, result[1]
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞
        report_success, report_time = test_report_generation()
        
        # –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è features
        analyze_feature_usage(model, feature_importance)
        
        # SHAP –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if enhanced_df is not None and len(enhanced_df) > 0:
            try:
                print("\nüîç –ó–ê–ü–£–°–ö SHAP –ê–ù–ê–õ–ò–ó–ê")
                print("=" * 50)
                from shap_analysis import run_comprehensive_feature_analysis
                shap_report = run_comprehensive_feature_analysis(enhanced_df.head(500))
                if shap_report:
                    print("  ‚úÖ SHAP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"  ‚ö†Ô∏è SHAP –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞
        print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        print("=" * 60)
        print(f"  ‚è±Ô∏è –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {load_time:.2f} —Å–µ–∫")
        print(f"  ‚è±Ô∏è Feature engineering: {fe_time:.2f} —Å–µ–∫") 
        print(f"  ‚è±Ô∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {train_time:.2f} —Å–µ–∫")
        print(f"  ‚è±Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞: {report_time:.2f} —Å–µ–∫")
        print(f"  üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤: {total_size:.1f} MB")
        print(f"  üíæ –ü–∏–∫–æ–≤–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {get_memory_usage():.1f} MB")
        
        total_time = load_time + fe_time + train_time + report_time
        
        print(f"\n‚è±Ô∏è –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        if total_time > 300:  # 5 –º–∏–Ω—É—Ç
            print("  ‚ö†Ô∏è –ú–ï–î–õ–ï–ù–ù–û: –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ –Ω–∞ Replit")
        elif total_time > 120:  # 2 –º–∏–Ω—É—Ç—ã
            print("  üí° –ü–†–ò–ï–ú–õ–ï–ú–û: –ù–æ —Å—Ç–æ–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å")
        else:
            print("  ‚úÖ –ë–´–°–¢–†–û: –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    performance_recommendations()
    
    print("\nüéØ –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main()