"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–∏
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib
import logging
from datetime import datetime
import os

from config import MODEL_PARAMS, MODEL_PATH, SCALER_PATH, CV_FOLDS, MIN_R2_SCORE, RANDOM_STATE
try:
    from main.data_integration import load_data_with_all_features
except ImportError:
    from data_integration import load_data_with_all_features
try:
    from main.data_integration import prepare_features_with_all_enhancements
except ImportError:
    from data_integration import prepare_features_with_all_enhancements

logger = logging.getLogger(__name__)

class SalesPredictor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø—Ä–æ–¥–∞–∂"""
    
    def __init__(self, model_type='random_forest'):
        self.model_type = model_type
        self.model = None
        self.feature_engineer = FeatureEngineer()
        self.is_trained = False
        self.feature_names = []
        self.training_metrics = {}
        
    def _create_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
        if self.model_type == 'random_forest':
            return RandomForestRegressor(**MODEL_PARAMS)
        elif self.model_type == 'gradient_boosting':
            return GradientBoostingRegressor(
                n_estimators=MODEL_PARAMS['n_estimators'],
                max_depth=MODEL_PARAMS['max_depth'],
                random_state=MODEL_PARAMS['random_state']
            )
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")
    
    def train(self, df, target_col='target', optimize_hyperparams=False):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {self.model_type}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = self.feature_engineer.prepare_data_for_training(
            df, target_col, scale=False  # Random Forest –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        )
        
        self.feature_names = self.feature_engineer.feature_names
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        if optimize_hyperparams:
            self.model = self._optimize_hyperparameters(X_train, y_train)
        else:
            self.model = self._create_model()
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        logger.info("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        self.model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        self._evaluate_model(X_train, X_test, y_train, y_test)
        
        self.is_trained = True
        logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
        
        return self.training_metrics
    
    def _optimize_hyperparameters(self, X_train, y_train):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        logger.info("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        
        if self.model_type == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
        else:
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2]
            }
        
        base_model = self._create_model()
        
        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=3,
            scoring='r2',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        logger.info(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        logger.info(f"–õ—É—á—à–∏–π R2 score: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def _evaluate_model(self, X_train, X_test, y_train, y_test):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        logger.info("–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_train_pred = self.model.predict(X_train)
        y_test_pred = self.model.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
        train_r2 = r2_score(y_train, y_train_pred)
        train_mse = mean_squared_error(y_train, y_train_pred)
        train_mae = mean_absolute_error(y_train, y_train_pred)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        test_r2 = r2_score(y_test, y_test_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=CV_FOLDS, scoring='r2')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.training_metrics = {
            'train_r2': train_r2,
            'train_mse': train_mse,
            'train_mae': train_mae,
            'test_r2': test_r2,
            'test_mse': test_mse,
            'test_mae': test_mae,
            'cv_mean_r2': cv_scores.mean(),
            'cv_std_r2': cv_scores.std(),
            'feature_count': len(self.feature_names)
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è ===")
        logger.info(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ R¬≤: {train_r2:.4f}")
        logger.info(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ R¬≤: {test_r2:.4f}")
        logger.info(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        logger.info(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ MSE: {test_mse:.2f}")
        logger.info(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ MAE: {test_mae:.2f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if test_r2 < MIN_R2_SCORE:
            logger.warning(f"R¬≤ score ({test_r2:.4f}) –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ ({MIN_R2_SCORE})")
        else:
            logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞ (R¬≤ = {test_r2:.4f})")
    
    def get_feature_importance(self, top_n=20):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_trained:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return None
        
        if not hasattr(self.model, 'feature_importances_'):
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances_")
            return None
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance_df.head(top_n)
    
    def predict(self, df, restaurant_name, date):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –∏ –¥–∞—Ç—ã"""
        if not self.is_trained:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–∑–µ—Ü
        X_sample = self.feature_engineer.prepare_single_sample(df, restaurant_name, date)
        
        if X_sample is None:
            return None
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
        prediction = self.model.predict(X_sample)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏
        sample_data = df[
            (df['restaurant_name'] == restaurant_name) & 
            (df['date'] == date)
        ]
        
        if not sample_data.empty:
            actual_sales = sample_data['total_sales'].iloc[0]
            predicted_sales = actual_sales + prediction  # prediction —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        else:
            actual_sales = None
            predicted_sales = None
        
        return {
            'prediction': prediction,
            'actual_sales': actual_sales,
            'predicted_sales': predicted_sales,
            'restaurant': restaurant_name,
            'date': date
        }
    
    def save_model(self, model_path=None, scaler_path=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if not self.is_trained:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return False
        
        model_path = model_path or MODEL_PATH
        scaler_path = scaler_path or SCALER_PATH
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            os.makedirs(os.path.dirname(scaler_path), exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            model_data = {
                'model': self.model,
                'feature_engineer': self.feature_engineer,
                'feature_names': self.feature_names,
                'model_type': self.model_type,
                'training_metrics': self.training_metrics,
                'trained_at': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, model_path)
            logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def load_model(self, model_path=None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_path = model_path or MODEL_PATH
        metadata_path = "models/client_model_metadata.json"  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model_data = joblib.load(model_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å –∏–ª–∏ –∫–∞–∫ –æ–±—ä–µ–∫—Ç sklearn
            if isinstance(model_data, dict):
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–ª–æ–≤–∞—Ä—å
                self.model = model_data['model']
                self.feature_engineer = model_data['feature_engineer']
                self.feature_names = model_data['feature_names']
                self.model_type = model_data['model_type']
                self.training_metrics = model_data.get('training_metrics', {})
            else:
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - –æ–±—ä–µ–∫—Ç sklearn
                self.model = model_data
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                if os.path.exists(metadata_path):
                    import json
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    
                    self.feature_names = metadata.get('feature_names', [])
                    self.model_type = metadata.get('model_type', 'random_forest')
                    self.training_metrics = metadata.get('metrics', {})
                    
                    # –°–æ–∑–¥–∞–µ–º feature_engineer —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    self.feature_engineer = FeatureEngineer()
                    self.feature_engineer.feature_names = self.feature_names
                else:
                    logger.warning(f"–§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö {metadata_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    self.feature_names = []
                    self.model_type = 'random_forest'
                    self.training_metrics = {}
                    self.feature_engineer = FeatureEngineer()
            
            self.is_trained = True
            
            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            if self.training_metrics:
                logger.info(f"R¬≤ score: {self.training_metrics.get('test_r2', 'N/A')}")
                logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def retrain(self, df, target_col='target'):
        """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        if self.is_trained:
            logger.info("–ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞, –≤—ã–ø–æ–ª–Ω—è—é –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ...")
        
        return self.train(df, target_col)
    
    def get_model_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        if not self.is_trained:
            return {"status": "not_trained"}
        
        return {
            "status": "trained",
            "model_type": self.model_type,
            "feature_count": len(self.feature_names),
            "metrics": self.training_metrics
        }

def train_sales_model(start_date=None, end_date=None, model_type='random_forest', 
                     optimize_hyperparams=False, save_model=True):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    logger.info("–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–¥–∞–∂")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
    logger.info("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π...")
    df = load_data_with_all_features()
    if df is None or df.empty:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return None
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ features
    logger.info("üåü –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö features...")
    df = prepare_features_with_all_enhancements(df)
    logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df.columns)} –ø–æ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    predictor = SalesPredictor(model_type)
    metrics = predictor.train(df, optimize_hyperparams=optimize_hyperparams)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    if save_model:
        predictor.save_model()
    
    # –í—ã–≤–æ–¥–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance = predictor.get_feature_importance()
    if importance is not None:
        logger.info("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for _, row in importance.head(10).iterrows():
            logger.info(f"  {row['feature']}: {row['importance']:.4f}")
    
    return predictor

def load_trained_model(model_path=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    predictor = SalesPredictor()
    if predictor.load_model(model_path):
        return predictor
    return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import sys
    sys.path.append('.')
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor = train_sales_model(
        model_type='random_forest',
        optimize_hyperparams=False,
        save_model=True
    )
    
    if predictor:
        print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
        print(f"–ú–µ—Ç—Ä–∏–∫–∏: {predictor.training_metrics}")
    else:
        print("–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")